\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{amsthm}
\usepackage{thmtools}
\usepackage[dvipsnames]{xcolor}
\declaretheorem[name=Théorème, style=plain, shaded={bgcolor=Lavender!30}]{thm}
\declaretheorem[name=Proposition, style=plain]{prop}
\declaretheorem[name=Lemme, style=plain]{lemme}
\declaretheorem[name=Définition, style=definition]{defn}
\declaretheorem[name=Exemple, style=definition]{exmp}
\declaretheorem[name=Remarque, style=remark, numbered=no]{remark}

\title{Géométrie I}
\author{C. B.}
\date{Semestre d'automne 2019}

\numberwithin{equation}{section}

\renewcommand{\labelitemi}{$\bullet$}
\renewcommand{\labelitemii}{$\cdot$}
\renewcommand{\phi}{\varphi}
\providecommand{\implied}{\longleftarrow}

\providecommand{\plan}{{\mathbb R^2}}
\providecommand{\origin}{{\vec 0}}
\providecommand{\id}{{\textnormal{Id}}}
\providecommand{\IdR}{{\id_\plan}}
\providecommand{\MR}{{M_2(\mathbb R)}}
\providecommand{\transR}{{\textnormal{T}(\plan)}}
\providecommand{\isom}{{\textnormal{Isom}(\plan)}}
\providecommand{\isomo}{{\isom_0}}
\providecommand{\isomop}{\isom_0^+}
\providecommand{\isomom}{\isom_0^-}
\providecommand{\unitcircle}{{\mathbb C^1}}
\providecommand{\mortho}[1]{{\textnormal{O}(#1)}}
\providecommand{\morthoR}{\mortho{\mathbb R}}
\providecommand{\mspec}[1]{{\textnormal{SO}_2(#1)}}
\providecommand{\mnspec}[1]{{\textnormal{O}_2(#1)^-}}
\providecommand{\mspecR}{\mspec{\mathbb R}}
\providecommand{\mnspecR}{\mnspec{\mathbb R}}
\providecommand{\ker}{{\textnormal{ker}}}
\providecommand{\Fix}{{\textnormal{Fix}}}
\providecommand{\trans}[1]{{t_{#1}}}
\providecommand{\bij}[1]{{\textnormal{Bij}(#1)}}
\providecommand{\bijR}{{\bij{\plan}}}
\providecommand{\lin}{{\textnormal{lin}}}
\providecommand{\longueur}[1]{{\lVert#1\rVert}}
\providecommand{\abs}[1]{{\lvert#1\rvert}}
\providecommand{\scalaire}[1]{{\langle#1\rangle}}
\providecommand{\ad}{{\textnormal{ad}}}
\providecommand{\st}{\mid}
\providecommand{\subgroupeq}{\leq}
\providecommand{\subgroupnormaleq}{\trianglelefteq}

% margins
\usepackage{geometry}
\geometry{left=1.5cm, right=1.5cm, top=1.5cm, bottom=1.5cm}

\begin{document}
\maketitle

\chapter*{Ennuis préliminaires}

Ce premier chapitre a pour but de définir quelques notions formelles dont nous aurons besoin afin de pouvoir explorer les vraies notions intéressantes de manière rigoureuse. Nous le garderons de longueur mininale afin de ne pas nous ennuyer, car vraiment, quel ennui! Pensez-y comme la préparation de votre sac à dos avant le départ vers cette forêt de connaissances que nous explorerons bientôt, afin d'en apprécier les plus belles clairières et les plus magnifiques détails: c'est là une étape nécessaire, mais certainement pas celle à laquelle nous nous adonnerons avec le plus d'excitation.

\begin{defn}
	Le \textbf{plan réel} est le produit cartésien $\mathbb R \times \mathbb R = \plan$. $\origin = (0, 0)$ est l'\textbf{origine}.\par
	$\plan$ est muni d'une structure de groupe :
	\begin{itemize}
		\item $+$: $(x, y) + (x', y') = (x+x', y+y')$
		\item $\origin$: $(x, y) + \origin = (x, y)$
		\item $-$: $-(x, y) = (-x, y), (x, y) + (-(x, y)) = \origin$
	\end{itemize}
	On peut également voir $\plan$ comme un groupe de translations: à $(x, y) = \vec u \in \plan$ on associe l'application
	\begin{align*}
		\trans{\vec u} = \trans{(x, y)} : \plan \to \plan,\\ (a, b) \mapsto (a, b) + (x, y)
	\end{align*}
	$\trans{\vec u}$ admet quelques propriétés:
	\begin{itemize}
		\item $\trans{\vec u}$ est une application bijective d'inverse $\trans{\vec u}^{-1} = \trans{-\vec u}$ et on a $\trans{\vec u} \circ \trans{-\vec u} = \IdR$
		\item $\trans{\vec u} \circ \trans{\vec v} = \trans{\vec u + \vec v}$
	\end{itemize}
\end{defn}

\begin{prop}
	L'application $t : \plan \to \bijR$ est un morphisme de groupes.
	\begin{proof}
		En fait, cela a déjà été montré plus haut, lorsqu'on examinait les propriétés des translations $\trans{\vec v}$.
	\end{proof}
\end{prop}

\begin{defn}
	La \textbf{longueur euclidienne} d'un vecteur $\vec v = (x, y)$ est donnée par
	\begin{align*}
		\longueur{\cdot} : \plan \to \mathbb R\\
		(x, y) \mapsto (x^2 + y^2)^{\frac12}.
	\end{align*}
	La \textbf{distance euclidiennt} entre deux vecteurs est donnée par
	\begin{align*}
		d : \plan \times \plan \to \mathbb R\\
		(\vec u, \vec v) \mapsto \longueur{\vec u - \vec v}.
	\end{align*}
\end{defn}

\begin{prop}
	La longueur $\longueur{\cdot}$ et la distance $d$ ont les propriétés suivantes:
	\begin{itemize}
		\item \textbf{séparation}:
			\begin{enumerate}
				\item $\longueur{\vec u} = 0 \iff \vec u = \origin$
				\item $d(\vec u, \vec v) = 0 \iff \vec u = \vec v$
			\end{enumerate}
		\item \textbf{symétrie}:
			\begin{enumerate}
				\item $\longueur{\vec u} = \longueur{-\vec u}$
				\item $d(\vec u, \vec v) = d(\vec v, \vec u)$
			\end{enumerate}
		\item \textbf{inégalité triangulaire}: 
			\begin{enumerate}
				\item $\longueur{\vec u + \vec v} \leq \longueur{\vec u} + \longueur{\vec v}$
				\item $d(\vec u, \vec v) \leq d(\vec u, \vec w) + d(\vec w, \vec v)$
			\end{enumerate}
		\item \textbf{homogénéité}: $\forall \lambda \in \mathbb R$ on a
			\begin{enumerate}
				\item $\longueur{\lambda \vec u} = \abs{\lambda}\longueur{\vec u}$
				\item $d(\lambda \vec u, \lambda \vec v) = \abs{\lambda}d(\vec u, \vec v)$
			\end{enumerate}
	\end{itemize}
\end{prop}
Les trois premières propriétés satisfaites par $d$ servent à définir la notion abstraite de ``distance'' sur un ensemble arbitraire $X$: c'est une application
	\begin{equation*}
		d: X \times X \to \mathbb R_{\geq 0}
	\end{equation*}
	vérifiant séparation, symétrie et inégalité triangulaire.

\begin{proof}[Démonstration (inégalité triangulaire).]
	La démonstration est trop chiante pour que je la recopie ici. J'ai des choses à faire moi.\\
	Elle inclut cependant la preuve de l'inégalité de Cauchy-Schwartz, qui est importante lors de la définition de distances, donc cherchez dans vos notes.
\end{proof}

\begin{defn}
	On définit l'application \textbf{produit scalaire}
	\begin{align*}
		\scalaire{\cdot, \cdot}: \plan \times \plan \to \mathbb R\\
		((a, b), (c, d)) \mapsto a \cdot c + b \cdot d
	\end{align*}
\end{defn}

\begin{prop}
	Le produit scalaire a les propriétés suivantes:
	\begin{itemize}
		\item \textbf{symétrie}: $\scalaire{\vec u, \vec v} = \scalaire{\vec v, \vec u}$
		\item \textbf{bilinéarité}: pour tous $\lambda, \mu \in \mathbb R$ on a $\scalaire{\lambda\vec u + \mu\vec v, \vec w} = \lambda\scalaire{\vec u, \vec w} + \mu\scalaire{\vec v, \vec w}$
		\item \textbf{défini positif}: $\scalaire{\vec u, \vec u} \geq 0$. De plus $\scalaire{\vec u, \vec u} = 0 \iff \vec u = \origin$
	\end{itemize}
	On remarque aussi que $\scalaire{\vec u, \vec u} = \longueur{\vec u}^2$.
\end{prop}

\begin{defn}
	On dit que deux vecteurs $\vec u$ et $\vec v$ sont \textbf{perpendiculaires} (ou \textbf{orthogonaux}) ssi $\scalaire{\vec u,  \vec v} = 0$.
\end{defn}

\begin{prop}
	Soient $\vec u, \vec v \in \plan \setminus \{\origin\}$ perpendiculaires. Alors tout vecteur $\vec w \in \plan$ se décompose de manière unique comme combinaison linéaire de $\vec u$ et $\vec v$, càd $\exists! \lambda, \mu \in \mathbb R$ t.q. $\vec w = \lambda \vec u + \mu \vec v$. De plus, $\lambda$ et $\mu$ sont donnés par les formules
	\begin{equation*}
		\lambda = \frac{\scalaire{\vec w, \vec u}}{\scalaire{\vec u, \vec u}} \text{ et } \mu = \frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}}.
	\end{equation*}
	On dit que $(\vec u, \vec v)$ est une \textbf{base orthogonale} de $\plan$.
\end{prop}
\begin{proof}
	Supposons que $\vec w = \lambda \vec u + \mu \vec v$, avec $\vec u$ et $\vec v$ orthogonaux non-nuls $\implies \scalaire{\vec u, \vec v} = 0$. Alors
	\begin{align*}
	\scalaire{\vec w, \vec u} &= \scalaire{\lambda \vec u + \mu \vec v, \vec u}\\
	&= \lambda\scalaire{\vec u, \vec u} + \mu\scalaire{\vec u, \vec v}\\
	&= \lambda\scalaire{\vec u, \vec u}
	\end{align*}
	$\iff \lambda = \dfrac{\scalaire{\vec w, \vec u}}{\scalaire{\vec u, \vec u}}$. On procède de la même manière pour trouver l'expression de $\mu$. Ces deux expressions sont uniques, et la décomposition de $\vec w$ sur $(\vec u, \vec v)$ est donc unique.\\
	On voit également que si $\longueur{\vec u} = \longueur{\vec v} = 1$, les expression se simplifient en $\lambda = \scalaire{\vec w, \vec u}$ et $\mu = \scalaire{\vec w, \vec v}$. Dans ce cas-ci, on appelle $(\vec u, \vec v)$ une \textbf{base orthonormée} (B.O.N).
\end{proof}

\chapter{Isométries du plan}

\section{Isométries, qu'êtes-vous donc?}
Maintenant que l'on a défini le plan en tant qu'objet mathématique, ainsi que quelques éléments de sa structure, on peut passer aux définitions plus intéressantes. Ainsi, la très ennuyeuse section précédente ne servait que de base formelle (essentielle) à la définition de la notion qui va nous occuper pendant toute l'année: celle d'isométrie. Comment peut-on donc définir ce type de transformations ?

\begin{defn}
	Une \textbf{isométrie} $\phi$ de $\plan$ est une application $\phi : \plan \to \plan$ qui préserve la distance euclidienne. Mathématiquement, $\phi$ est une isométrie de $\plan$ ssi $\forall P, Q \in \plan$, on a 
	\begin{equation}
		d(\phi(P), \phi(Q)) = d(P, Q).
	\end{equation}
	On désignera l'ensemble des isométries de $\plan$ par $\isom$.
\end{defn}

Cette définition suit parfaitement l'intuition visuelle que l'on aurait d'une isométrie: une transformation du plan qui ne modifie pas les distances est, visuellement, une transformation qui n'``écrase'' ni n'``étire'' ni ne ``tord'' rien.\par
Rien qu'en ayant posé cette définition simple et intuitive, on a ouvert le chemin à l'exploration formelle d'une forêt de connaissances qui ne demande qu'à être explorée; nombre de propriétés que l'on pourrait conjecturer en visualisant l'action de telles transformations, donc en observant la forêt de l'extérieur, seront rigoureusement démontrables dans le cadre de cette définition et de celles qui suivront.\par
Nous pouvons donc nous poser la question : quelles applications que nous connaissons déjà sont des isométries ?

\begin{exmp}
	$\IdR \in \isom$, car $d(\IdR(P), \IdR(Q)) = d(P, Q)$.
\end{exmp}

\begin{prop}
	Soit $\vec v \in \plan$. Alors $\trans{\vec v}$ est une isométrie $( \iff \trans{\vec v} \in \isom)$.
\end{prop}
\begin{proof}
	$\forall P, Q \in \plan$, on a
	\begin{align*}
		d(\trans{\vec v}(P), \trans{\vec v}(Q)) &= \longueur{P + \vec v - (Q + \vec v)}\\
		&= \longueur{P - Q + \vec v - \vec v}\\
		&= \longueur{P - Q}\\
		&= d(P, Q).
	\end{align*}
	Donc $\trans{\vec v} \in \isom$.
\end{proof}

Les translations, ainsi que l'identité (qui n'est en fait que la translation de vecteur $\origin$), font donc partie des isométries du plan; quand on visualise ça dans notre tête, ça paraît bien évident: l'identité ne change rien et conserve donc les distances, et les translations ne font que ``déplacer'' tous les points du plan de la même manière, sans modifier donc les distances entre eux. Mais nous avons là fait un grand pas en prouvant de manière formelle que ces transformations sont bien des isométries! Cet accord entre le résultat formel et notre intuition nous indique déjà que nous sommes probablement sur la bonne voie, que nous avons posé une définition formelle des isométries qui va dans le sens de nos intuitions, et donc que cette définition nous aidera probablement à prouver d'autres résultats qui, jusqu'ici, semblaient hors de portée de la rigueur mathématique.\par
La question se pose maintenant: comment avancer plus loin dans notre exploration ? Nous sommes arrivé·e·s à un croisement qui nous offre tant de possibilités, et il faut que nous fassions un choix dans les définitions que nous allons poser et qui nous orienteront vers de nouvelles clairières et cascades. Oh, j'ai une idée! Pourquoi pas\dots

\section{Les isométries préservant l'origine}

\begin{defn}
	On notera $\isomo$ l'ensemble des isométries qui préservent l'origine, c'est-à-dire, mathématiquement,
	\begin{equation*}
		\isomo = \{\phi \in \isom \st \phi(\origin) = \origin\}.
	\end{equation*}
\end{defn}

N'est-ce pas là une belle définition ? Peut-être les isométries préservant l'origine nous cachent-elles des secrets\dots Elles ont tout au moins la beauté d'être facilement imaginables visuellement, et nous pourrons donc probablement tenter de formaliser les intuitions qui nous viendront en tête! Nous nous enfonçons donc dans la forêt en suivant ce premier sentier, sans trop savoir où cela va nous mener\dots\par

Voici qu'après quelques pas déjà se profile une intuition, que j'avais eue (mais j'ai l'avantage d'être rédacteur·e de cette aventure) avant même d'avoir posé la définition sur papier. Une isométrie conservant l'origine ne serait-elle pas linéaire ? Pour l'instant, je n'arrive à m'imaginer que des rotations et des symétries, ainsi que l'identité bien sûr, qui auraient toutes la propriété apparente de préserver la longueur, ainsi que le produit scalaire\dots

\begin{lemme}
	Soit $\phi \in \isomo$. Alors $\phi$ préserve la longueur et le produit scalaire.
\end{lemme}
\begin{proof}
	$\forall \vec v \in \plan$, on a
	\begin{align*}
		\longueur{\phi(\vec v)} &= d(\origin, \phi(\vec v))\\
		&= d(\phi(\origin), \phi(\vec v)) = d(\origin, \vec v)\\
		&= \longueur{\vec v},
	\end{align*}
	donc $\phi$ préserve bien la longueur de tout vecteur.
	Pour ce qui est du produit scalaire:
	\begin{align*}
		\scalaire{\phi(\vec u), \phi(\vec v)} &= \frac12\bigl( \longueur{\phi(\vec u)}^2 + \longueur{\phi(\vec v)}^2 - \longueur{\phi(\vec u) - \phi(\vec v)}^2 \bigr)\\
		&= \frac12\bigl( \longueur{\vec u}^2 + \longueur{\vec v}^2 - d(\phi(\vec u), \phi(\vec v))^2 \bigr)\\
		&= \frac12\bigl( \longueur{\vec u}^2 + \longueur{\vec v}^2 - d(\vec u, \vec v)^2 \bigr)\\
		&= \frac12\bigl( \longueur{\vec u}^2 + \longueur{\vec v}^2 - \longueur{\vec u - \vec v}^2 \bigr)\\
		&= \scalaire{\vec u, \vec v}.
	\end{align*}
\end{proof}

Cela paraît peut-être ambitieux, mais j'ai bien l'impression que les rotations et symétries (et l'identité, qui en fait est une rotation d'angle nul), qui constituent selon mon intuition les seuls éléments de $\isomo$, sont toutes des applications linéaires. Voyons voir si ce résultat se tient:
\begin{thm}
	Toute isométrie préservant l'origine est linéaire.
\end{thm}
\begin{proof}
	Il suffit de montrer que $\forall \phi \in \isomo, \phi(\lambda\vec u + \vec v) = \lambda\phi(\vec u) + \phi(\vec v)$. Or
	\begin{align*}
		&\phi(\lambda\vec u + \vec v) = \lambda\phi(\vec u) + \phi(\vec v)\\
		\iff &\longueur{\phi(\lambda\vec u + \vec v) - (\lambda\phi(\vec u) + \phi(\vec v))} = 0,
		\intertext{(par la propriété de séparation de la distance/longueur), ce qui est équivalent à l'équation}
		&\longueur{\phi(\lambda\vec u + \vec v) - (\lambda\phi(\vec u) + \phi(\vec v))}^2 = 0
	\end{align*}
	Cependant
	\begin{align*}
		&\longueur{\phi(\lambda\vec u + \vec v) - (\lambda\phi(\vec u) + \phi(\vec v))} = \scalaire{\phi(\lambda \vec u + \vec v) - \lambda \phi(\vec u) - \phi(\vec v), \phi(\lambda \vec u + \vec v) - \lambda \phi(\vec u) - \phi(\vec v)}\\
		= &\scalaire{\phi(\lambda \vec u + \vec v), \phi(\lambda \vec u + \vec v) - \lambda \phi(\vec u) - \phi(\vec v)} - \lambda\scalaire{\phi(\vec u), \phi(\lambda \vec u + \vec v) - \lambda \phi(\vec u) - \phi(\vec v)} - \scalaire{\phi(\vec v), \phi(\lambda \vec u + \vec v) - \lambda \phi(\vec u) - \phi(\vec v)}\\
		= &\longueur{\phi(\lambda \vec u + \vec v)}^2 - \lambda^2\longueur{\phi(\vec u)}^2 + \longueur{\phi(\vec v)}^2 - 2\lambda\scalaire{\phi(\lambda \vec u + \vec v), \phi(\vec u)} - 2\scalaire{\phi(\lambda\vec u + \vec v), \phi(\vec v)} + 2\lambda\scalaire{\phi(\vec u), \phi(\vec v)}\\
	\intertext{(par le lemme)}
		= &\longueur{\lambda \vec u + \vec v}^2 - \lambda^2\longueur{\vec u}^2 + \longueur{\vec v}^2 - 2\lambda\scalaire{\lambda \vec u + \vec v, \vec u} - 2\scalaire{\lambda \vec u + \vec v, \vec v} + 2\lambda\scalaire{\vec u, \vec v}\\
		= &\longueur{\lambda \vec u + \vec v - \lambda \vec u - \vec v}^2\\
		= &\longueur{\origin}^2\\
		= &0.
	\end{align*}
	Donc, les isométries préservant l'origine sont bien linéaires, c'est pourquoi nous les appellerons souvent plutôt \textbf{isométries linéaires}.
\end{proof}

\begin{thm}
	Soit $\phi \in \isomo$. Alors $\phi$ est bijective, et $\phi^{-1} \in \isomo$. Ainsi, $\isomo$ est muni d'une structure de groupe et $\isomo \subgroupeq \textnormal{GL}(\plan)$ (où $\textnormal{GL}(\plan)$ est le groupe linéaire de $\plan$, l'ensemble des applications linéaires inversibles $\plan \to \plan$).
\end{thm}
\begin{proof}
	Pour montrer l'injectivité, il suffit de montrer que $\ker(\phi) = \{\origin\}$. Supposons $\vec v \in \ker(\phi)$. Alors
	\begin{equation*}
		\phi(\vec v) = \origin \iff \longueur{\phi(\vec v)} = 0 \implies \longueur{\vec v} = 0 \iff \vec v = \origin.
	\end{equation*}
	La surjectivité peut être déduite directement par le théorème du rang, un résultat d'algèbre linéaire. Cependant on peut aussi la prouver directement, c'est-à-dire montrer que $\forall \vec v \in \plan \exists \vec u \in \plan$ t.q. $\phi(\vec u) = \vec v$. On pose la base canonique $B = (e_1, e_2)$ où $e_1 = (1, 0)$ et $e_2 = (0, 1)$. C'est une base orthonormée. Donc $\exists \alpha, \beta \in \mathbb R$ t.q. $\vec v = \alpha \phi(e_1) + \beta \phi(e_2)$. Par la linéarité de $\phi$, $\vec v = \phi(\alpha e_1) + \phi(\beta e_2) = \phi(\alpha e_1 + \beta e_2)$. En posant $\vec u = \alpha e_1 + \beta e_2$, on a trouvé le $\vec u$ recherché, donc $\phi$ est surjective.\\
	Comme $\phi$ est injective et surjective, $\phi$ est bijective.\par
	Il reste maintenant à montrer que $\phi^{-1} \in \isomo$, c.à.d que $\phi^{-1}$ préserve les distances et préserve l'origine. On sait que $\phi(\origin) = \origin \implies \phi^{-1}(\origin) = \origin$. De plus, $d(\phi^{-1}(\vec u), \phi^{-1}(\vec v)) = d(\phi(\phi^{-1}(\vec u)), \phi(\phi^{-1}(\vec v))) = d(\vec u, \vec v)$. Donc $\phi^{-1} \in \isomo$.\par
	Finalement, montrons grâce aux résultats précédents que $\isomo \subgroupeq \textnormal{GL}(\plan)$:
	\begin{itemize}
		\item $\IdR \in \isomo$;
		\item $\phi, \psi \in \isomo \implies \phi \circ \psi(\origin) = \phi(\phi(\origin)) = \phi(\origin) = \origin \implies \phi \circ \psi \in \isomo$;
		\item $\phi \in \isomo \implies \phi$ est bijective et $\phi^{-1} \in \isomo$.
	\end{itemize}
	Donc, $\isomo \subgroupeq \textnormal{GL}(\plan)$.
\end{proof}

\begin{thm}
	On a les relations suivantes:
	\begin{enumerate}
		\item $\transR, \isom$ et $\isomo \subgroupeq \bijR$;
		\item $\isom = \transR \circ \isomo$, c.à.d que $\forall \phi \in \isom \exists t \in \transR, \phi_0 \in \isomo$ t.q. $\phi = t \circ \phi_0$. De plus, $t$ et $\phi_0$ sont uniques;
		\item $\transR \subgroupnormaleq \isom$.
	\end{enumerate}
\end{thm}

\begin{proof}
	Nous allons prouver les trois points dans l'ordre.
	\begin{enumerate}
		\item Nous avons déjà montré que $\transR$ et $\isomo \subgroupeq \bijR$. Il reste donc à montrer que $\isom \subgroupeq \bijR$:
		\begin{itemize}
			\item $\IdR \in \isom$;
			\item soient $\phi, \psi \in \isom$. Alors $\phi \circ \psi \in \isom$: $d(\phi \circ \psi(\vec u), \phi \circ \psi(\vec v)) = d(\phi(\vec u), \phi(\vec v)) = d(\vec u, \vec v)$;
			\item il faut montrer que $\forall \phi \in \isom$, $\phi$ est bijective et $\phi^{-1} \in \isom$. Ce sera un corollaire du troisième point.
		\end{itemize}
		
		\item `PREUVE À COMPLETER'
		
		\item On veut montrer que $\transR \subgroupnormaleq \isom$, c.à.d que $\forall \vec u \in \plan, \forall \phi \in \isom, \ad(\phi)(\trans{\vec u}) = \phi \circ \trans{\vec u} \circ \phi^{-1} \in \transR$.
		\begin{itemize}
			\item dans le cas où $\phi = \phi_0 \in \isomo$, on a
				\begin{align*}
					\phi_0 \circ \trans{\vec u} \circ \phi_0^{-1}(\vec w) &= \phi_0(\trans{\vec u}(\phi_0^{-1}(\vec w)))\\
					&= \phi_0(\vec u + \phi_0^{-1}(\vec w))\\
					&= \phi_0(\vec u) + \phi_0(\phi_0^{-1}(\vec w))\\
					&= \phi_0(\vec u) + \vec w\\
					&= \trans{\phi_0(\vec u)}(\vec w),
				\end{align*}
				donc on retombe bien sur une translation.
			\item dans le cas général, on a
				\begin{align*}
					\ad(\phi)(\trans{\vec u}) &= \ad(\trans{\phi(\origin)} \circ \phi_0)(\trans{\vec u})\\
					&= \trans{\phi(\origin)} \circ \phi_0 \circ \trans{\vec u} \circ (\trans{\phi(\origin)} \circ \phi_0)^{-1} \\
					&= \trans{\phi(\origin)} \circ \underbrace{\phi_0 \circ \trans{\vec u} \circ \phi_0^{-1}}_{\ad(\phi_0)(\trans{\vec u})} \circ \trans{\phi(\origin)}^{-1}\\
					&= \trans{\phi(\origin)} \circ \trans{\vec v} \circ \trans{-\phi(\origin)}\\
					&= \trans{\phi(\origin) + \vec v - \phi(\origin)}\\
					&= \trans{\vec v},
				\end{align*}
				donc ici aussi on retombe bien sur une translation.
		\end{itemize}
		Comme $\forall \vec u \in \plan \forall \phi \in \isom, \ad(\phi)(\trans{\vec u}) \in \transR$, on a bien montré que $\transR \subgroupnormaleq \isom$.
	\end{enumerate}
\end{proof}

Le résultat de la décomposition de toute isométrie $\phi \in \isom$ en une composante linéaire $\phi_0 \in \isomo$ et une composante de translation $t \in \transR$ est très beau du point de vue de la structure mathématique des isométries du plan. Ce théorème est encore plus fort, parce qu'il nous donne la formule de ces deux parties. Cette formule est de plus assez intuitive: le fait que la translation soit de vecteur $\phi(\origin)$ s'explique par le fait que la partie linéaire ne modifie pas du tout l'origine $\origin$, et donc qu'il doit bien y avoir une autre composante qui s'occupe de la déplacer. Bref, tout ceci s'explique plus difficilement qu'il ne se comprend par l'exploration personnelle de ces concepts, c'est pourquoi je vous encourage à créer votre propre modèle mental pour vous expliquer ces choses-ci intuitivement au-delà de l'aspect purement rigoureux.\par
L'importance de cette décomposition motive la définition suivante\dots

\begin{defn}
	On définit l'application
	\begin{equation*}
		\lin: \begin{array}{l}
			\isom \to \isomo\\
			\phi \mapsto \phi_0
		\end{array}.
	\end{equation*}
	On appelle $\lin(\phi) = \phi_0$ la \textbf{partie linéaire} de $\phi$.
\end{defn}

\begin{thm}
	L'application $\lin$ est un morphisme de groupes surjectif de noyau $\ker(\lin) = \transR$.
\end{thm}
\begin{proof}
	La preuve de la surjectivité est facile: soit $\phi_0 \in \isomo$, alors comme $\isomo \subseteq \isom$, il suffit de choisir $\phi_0 \in \isom$ pour que $\lin(\phi_0) = \phi_0$.\par
	Pour ce qui est du fait que $\lin$ est un morphisme, il faut montrer que $\lin(\phi \circ \psi) = \lin(\phi) \circ \lin(\psi) = \phi_0 \circ \psi_0$:
	\begin{align*}
		\lin(\phi \circ \psi) &= \lin(\trans{} \circ \phi_0 \circ \trans{}' \circ \psi_0)\\
		&= \lin(\trans{} \circ \phi_0 \circ \trans{}' \circ \underbrace{\phi_0^{-1} \circ \phi_0}_{\IdR} \circ \psi_0)\\
		&= \lin(\trans{} \circ \underbrace{\phi_0 \circ \trans{}' \circ \phi_0^{-1}}_{\ad(\phi_9)(\trans{}') = \trans{}''} \circ \underbrace{\phi_0 \circ \psi_0}_{\chi_0})\\
		&= \lin(\trans{} \circ \trans{}'' \circ \chi_0)\\
		&= \lin(\trans{}''' \circ \chi_0)\\
		&= \chi_0\\
		&= \phi_0 \circ \psi_0\\
		&= \lin(\phi) \circ \lin(\psi).
	\end{align*}
	Finalement, soit $\phi \in \isom$ t.q. $\lin(\phi) = \IdR$. On a
	\begin{align*}
		\lin(\phi) = \lin(\trans{} \circ \phi_0) = \phi_0 = \IdR \implies \phi = \trans{} \implies \phi \in \transR,
	\end{align*}
	donc $\ker(\lin) = \transR$.
\end{proof}

Bon, maintenant que nous avons compris quelques unes des propriétés de base des isométries linéaires et leurs liens avec les isométries en général, il est intéressant de se rappeler que toute application linéaire peut être représentée par une matrice. Particulièrement, la matrice d'une application linéaire peut nous donner plus d'information sur son action géométrique sur le plan. À quoi ressemblent donc les matrices des isométries linéaires ?

\section{Voyage au coeur de la Matrice}
Soit $\phi \in \isomo$, et soit $B_0 = (e_1, e_2) = ((1, 0), (0, 1))$ la base canonique de $\plan$. Comme $\phi$ est linéaire, $\phi$ en tant qu'application est complètement déterminée par les valeurs de $\phi(e_1)$ et $\phi(e_2)$: en effet $(x, y) = x(1, 0) + y(0, 1) = xe_1 + ye_2$, et donc $\phi((x, y)) = \phi(xe_1 + ye_2) = x\phi(e_1) + y\phi(e_2)$.\par
De cette observation, on peut conclure que $\phi \in \isomo$ est complètement déterminée par une matrice $2 \times 2$. Renommons $\phi(e_1) = (a, c)$ et $\phi(e_2) = (b, d)$, avec $a, b, c, d \in \mathbb R$.\par
On peut définir une application bijective qui envoie $\phi$ sur une matrice:
\begin{equation*}
	\phi \mapsto M_\phi = \begin{pmatrix}
		a & b\\
		c & d
	\end{pmatrix}
	= \text{matrice associée à $\phi$ dans la base $B_0$}.
\end{equation*}
On a placé l'image de $e_1$ par $\phi$, donc $\phi(e_1) = e_1'$ dans la première colonne de la matrice, et $\phi(e_2) = e_2'$ dans la deuxième colonne. Ainsi, lorsqu'on multiplie un vecteur $(x, y)$ par cette matrice, on obtient

\begin{equation*}
	(x, y) \cdot \begin{pmatrix}
	a & b\\
	c & d
	\end{pmatrix}
	= \begin{pmatrix}
		xa + yb\\
		xc + yd
	\end{pmatrix}
	= x\begin{pmatrix} a\\c \end{pmatrix} + y\begin{pmatrix} b\\d \end{pmatrix}
	= x\phi(e_1) + y\phi(e_2)
	= \phi((x, y))
\end{equation*}
Multiplier le vecteur par la matrice de l'application revient donc à appliquer l'application au vecteur, ce qui est exactement ce que l'on désirait.\par
On remarque que si l'on choisit de représenter le vecteur $(x, y)$ comme un vecteur vertical $\left(\begin{smallmatrix} x\\y \end{smallmatrix}\right)$, il suffit d'inverser l'ordre du vecteur et de la matrice:	\begin{equation*}
	(x, y) \cdot \begin{pmatrix}
		a & b\\
		c & d
	\end{pmatrix}
	= \begin{pmatrix}
		a & b\\
		c & d
	\end{pmatrix} \cdot \begin{pmatrix}
		x\\y
	\end{pmatrix}.
\end{equation*}
On préférera utiliser la seconde forme, car elle apparaît visuellement de manière plus semblable aux applications: lorsqu'on compose des applications, on ajoute les applications à gauche et non à droite:
\begin{equation*}
	\phi \circ \psi\bigl((x, y)\bigr) = M_\phi \cdot M_\psi \cdot \begin{pmatrix}
		x\\y
	\end{pmatrix}
\end{equation*}
Cette matrice dépend de la base $B$ choisie. On travaillera le plus souvent avec $B_0$ pour des questions de simplicité, mais il vaut mieux garder en tête cette possibilité de changer de base, on sait jamais.\par
Comme les isométries linéaires sont entièrement déterminées par leur matrice, il est important que nous connaissions bien le fonctionnement de ces matrices.

\begin{defn}
	On nomme $M_{2 \times 2}(\mathbb R)$ ou $\MR$ l'ensemble des matrices $2 \times 2$ à coefficients dans $\mathbb R$. Ces matrices sont de la forme
	\begin{equation*}
		\begin{pmatrix}
			a & b\\
			c & d
		\end{pmatrix}
	\end{equation*}
\end{defn}
Toutes les matrices associées aux isométries linéaires seront dans $\MR$, comme vu plus haut. Quelle structure peut-on donc définir sur cet ensemble ?

\begin{prop}
	L'ensemble $\MR$, muni de l'addition matricielle $+$ et de la multiplication matricielle $\cdot$, forme un anneau unitaire.
\end{prop}
\begin{proof}
	Il faut vérifier les trois axiomes qui définissent un anneau unitaire:
	\begin{enumerate}
		\item $\MR$ est un groupe abélien;
		\item la multiplication $\cdot$ sur $\MR$ est associative, et il existe $1_\MR$ (l'unité multiplicative);
		\item la multiplication est distributive par rapport à l'addition.
	\end{enumerate}
	La preuve de ces trois points est directe, un peu fastidieuse et pas très intéressante. Nous nous contenterons de montrer que $1_\MR = \left(\begin{smallmatrix}
	1 & 0\\
	0 & 1
	\end{smallmatrix}\right)$:
	\begin{align*}
		\begin{pmatrix}
			a & b\\
			c & d
		\end{pmatrix} \cdot
		\begin{pmatrix}
			1 & 0\\
			0 & 1
		\end{pmatrix} =
		\begin{pmatrix}
			a \cdot 1 + c \cdot 0 & a \cdot 0 + b \cdot 1\\
			c \cdot 1 + d \cdot 0 & c \cdot 0 + d \cdot 1
		\end{pmatrix} =
		\begin{pmatrix}
			a & b\\
			c & d
		\end{pmatrix}.		
		\intertext{De même,}
		\begin{pmatrix}
			1 & 0\\
			0 & 1
		\end{pmatrix} \cdot
		\begin{pmatrix}
			a & b\\
			c & d
		\end{pmatrix} =
		\begin{pmatrix}
			1 \cdot a + 0 \cdot c & 0 \cdot a + 1 \cdot b\\
			1 \cdot c + 0 \cdot d & 0 \cdot c + 1 \cdot d
		\end{pmatrix} =
		\begin{pmatrix}
			a & b\\
			c & d
		\end{pmatrix}.
	\end{align*}
\end{proof}

\begin{defn}
	Soit $M = \left(\begin{smallmatrix}
		a & b\\
		c & d
	\end{smallmatrix}\right)$. On définit l'application \textbf{déterminant} par
	\begin{equation*}
		\det: \begin{array}{l}
			M_{2\times2}(\mathbb R) \to \mathbb R\\
			\left(\begin{smallmatrix}
				a & b\\
				c & d
			\end{smallmatrix}\right) \mapsto ad-bc
		\end{array}
	\end{equation*}
	Donc $\det(M) = ad-bc$ est le \textbf{déterminant} de $M$.
\end{defn}

\begin{lemme}[FAUX]
	L'application $\det : \bigl(\MR, +, \cdot\bigr) \to \bigl(\mathbb R, +, \cdot\bigr)$ est un morphisme d'anneaux.
\end{lemme}

\begin{proof}
	Il faut montrer vérifier les trois égalités suivantes $\forall M_1, M_2 \in \MR$
	\begin{enumerate}
		\item $\det(M_1 + M_2) = \det(M_1) + \det(M_2)$;
		\item $\det(M_1 \cdot M_2) = \det(M_1) \cdot \det(M_2)$;
		\item $\det(1_\MR) = 1$.
	\end{enumerate}
	Il s'avère en fait que la première égalité n'est pas vérifiée. La seconde et la troisième, cependant, le sont. Cela laisse penser que \textbf{si les éléments de $\MR$ étaient inversibles, on aurait un morphisme de groupes} $\bigl(\MR, \cdot\bigr) \to \bigl(\mathbb R, \cdot\bigr)$. Gardons cela en tête lors de nos avancées.
\end{proof}

\begin{thm}
	Soit $\phi \in \isomo$ et $M_\phi = \left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}\right)$ sa matrice associée. Alors $M_{\phi^{-1}} = \left(\begin{smallmatrix} a&c\\b&d \end{smallmatrix}\right)$ et les coefficients $a, b, c$ et $d$ vérifient
	\begin{equation*}
		\begin{cases}
			a^2 + c^2 = b^2 + d^2 = a^2 + b^2 = c^2 + d^2 = 1\\
			ab + cd = ac + bd = 0\\
			\det(M_\phi) = det(M_{\phi^{-1}}) = ad - bc = \pm 1
		\end{cases}
	\end{equation*}
\end{thm}

Pour prouver ceci, nous aurons besoin du lemme suivant:

\begin{lemme}[formule d'adjonction]
	Soient $\phi \in \isomo$, $\vec u, \vec v \in \plan$. Alors $\scalaire{\phi(\vec u), \vec v} = \scalaire{\vec u, \phi^{-1}(\vec v)}$.
\end{lemme}

\begin{proof}
	Soit $\phi \in \isomo$. Alors on sait que $\phi$ préserve le produit scalaire, donc $\scalaire{\vec u, \phi^{-1}(\vec v)} = \scalaire{\phi(\vec u), \phi(\phi^{-1}(\vec v))} = \scalaire{\phi(\vec u), \vec v}$.
\end{proof}

\begin{proof}[Démonstration du théorème.]
	Posons $\phi(e_1) = ae_1 + ce_2, \phi(e_2) = be_1 + de_2$, avec $B_0 = (e_1, e_2)$ la base canonique de $\plan$. Comme $B_0$ est orthonormée on a
	\begin{equation*}
		a = \scalaire{\phi(e_1), e_1}, \;
		c = \scalaire{\phi(e_1), e_2}, \;
		b = \scalaire{\phi(e_2), e_1}, \;
		d = \scalaire{\phi(e_2), e_2}.
	\end{equation*}
	Pareillement, avec $M_{\phi^{-1}} = \begin{smallmatrix} a'&b'\\c'&d' \end{smallmatrix}$:
	\begin{equation*}
		a' = \scalaire{\phi^{-1}(e_1), e_1}, \;
		c' = \scalaire{\phi^{-1}(e_1), e_2}, \;
		b' = \scalaire{\phi^{-1}(e_2), e_1}, \;
		d' = \scalaire{\phi^{-1}(e_2), e_2}.
	\end{equation*}
	Par la formule d'injonction du lemme précédent, on a
	\begin{equation*}
		a = \scalaire{\phi(e_1), e_1} = \scalaire{e_1, \phi^{-1}(e_1)} = \scalaire{\phi^{-1}(e_1), e_1} = a'.
	\end{equation*}
	De même, $d = d'$.\par
	Similairement,
	\begin{equation*}
		b = \scalaire{\phi(e_2), e_1} = \scalaire{e_2, \phi^{-1}(e_1)} = \scalaire{\phi^{-1}(e_1), e_2} = c'.
	\end{equation*}
	De même, $c = b'$. Donc,
	\begin{equation*}
		M_{\phi^{-1}} = \begin{pmatrix}
			a & c\\
			b & d
		\end{pmatrix}.
	\end{equation*}
	Il reste donc à montrer les relations entre coefficients de $M_\phi$. On remarque que
	\begin{equation*}
		\scalaire{\phi(e_1), \phi(e_1)} =
		\begin{cases}
			\scalaire{e_1, e_1} = 1\\
			\scalaire{ae_1 + ce_2, ae_1 + ce_2} = a^2\scalaire{e_1, e_1} + 2ac\scalaire{e_1, e_2} + c^2\scalaire{e_2, e_2} = a^2 + c^2
		\end{cases}
	\end{equation*}
	donc $a^2 + c^2 = 1$. De la même façon, $\scalaire{\phi(e_2), \phi(e_2)} = b^2 + d^2 = 1$. Aussi,
	\begin{equation*}
		\scalaire{\phi(e_1), \phi(e_2)} =
		\begin{cases}
			\scalaire{e_1, e_2} = 0\\
			\scalaire{ae_1 + ce_2, be_1 + de_2} = ab + cd
		\end{cases}
	\end{equation*}
	donc $ab + cd = 1$. De même, on trouve les 3 autres relations. Finalement, on a
	\begin{align*}
		\det(M_\phi)^2 &= (ad - bc)^2 = a^2d^2 - 2abcd + b^2c^2\\
		&= a^2d^2 + a^2c^2 + a^2c^2 + b^2c^2 = a^2(d^2 + c^2) + c^2(b^2 + a^2)\\
		&= a^2 + c^2 = 1
	\end{align*}
	$\implies \det(M_\phi) = \pm 1$.
\end{proof}

\begin{remark}
	Si l'on avait choisi une autre base de $\plan$ pour associer une matrice $M$ à une isométrie linéaire $\phi_0 \in \isomo$, la matrice aurait également satisfait ces propriétés.
\end{remark}

\begin{defn}
	Une matrice $M$ est dite \textbf{orthogonale} ssi elle est associée à une isométrie linéaire (dans la base canonique).
	\begin{itemize}
		\item si $\det M = 1$ la matrice est dite \textbf{spéciale} et l'isométrie associée aussi;
		\item si $\det M = -1$ la matrice est dite \textbf{non-spéciale} et l'isométrie associée aussi.		
	\end{itemize}
	On note $\morthoR$ l'ensemble des matrices orthogonales,
	\begin{itemize}
		\item $\mspecR$ l'ensemble des matrices spéciales et
		\item $\mnspecR$ l'ensemble des matrices non-spéciales.
	\end{itemize}
\end{defn}

\begin{prop}
	Soit $M = \left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}\right)$ t.q. $a^2 + c^2 = b^2 + d^2 = 1$ et $ab + cd = 0$, alors $M$ est orthogonale et $a, b, c$ et $d$ satisfont les propriétés du théorème.
\end{prop}
\begin{proof}
	Soit $\phi_0 \in \isomo$. Alors $\phi_0$ est telle que $\phi_0(e_1) = ae_1 + ce_2 = (a, c)$, $\phi_0(e_2) = be_1 + de_2 = (b, d)$. Alors $(\phi_0(e_1), \phi_0(e_2)$ est une base orthonormée de $\plan$.\par
	En effet:
	\begin{itemize}
		\item $\scalaire{\phi_0(e_1), \phi_0(e_1)} = \scalaire{(a, c), (a, c)} = a^2 + c^2 = 1$;
		\item $\scalaire{\phi_0(e_2), \phi_0(e_2)} = \scalaire{(b, d), (b, d)} = b^2 + d^2 = 1$;
		\item $\scalaire{\phi_0(e_1), \phi_0(e_2)} = \scalaire{(a, b), (c, d)} = ab + cd = 0$.
	\end{itemize}
	$\phi_0$ est donc une isométrie linéaire.
\end{proof}

Nous avons désormais établi un certain nombre de conditions et résultats sur les matrices associées à des appplications linéaires. Le résultat suivant classifie exactement toutes les matrices qui remplissent ces conditions dans deux classes, que nous étudierons pendant quelque temps.

\begin{prop}
	Soit $M$ une matrice $2 \times 2$. Alors $M$ est orthogonale ssi $\exists c, s \in \mathbb R$ tels que $c^2 + s^2 = 1$ et $M$ est d'une des deux formes suivantes:
	\begin{itemize}
		\item $M = \begin{pmatrix}
			\tilde c &-\tilde s \\ \tilde s & \tilde c
		\end{pmatrix}$ et $M$ est spéciale;
		\item $M = \begin{pmatrix}
			\tilde c & \tilde s \\ \tilde s & -\tilde c
		\end{pmatrix}$ et $M$ est non-spéciale.
	\end{itemize}
\end{prop}
	
\begin{proof}
	"$\implied$": si $M$ est de l'une des deux formes ci-dessus, $M = \left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}\right)$ en renommant $\tilde s$, $\tilde c$, $-\tilde s$ et $-\tilde c$ et il est clair que $a^2 + c^2 = b^2 + d^2 = 1$ et $ab + cd = 0$, ce qui implique que $M$ est orthogonale. \par
	"$\implies$": soit $M$ orthogonale, $M = \left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}\right)$. On pose $a = \tilde c, c = \tilde s$, et on a que $a^2 + c^2 = 1$. On sait que $\det M = ad - bc = \pm 1$.
	\begin{itemize}
		\item si $\det M = 1$, $ad - bc = 1 = \tilde cd + b\tilde s = 1$. De plus, comme $M$ est orthogonale, $ad + bc = \tilde cb + \tilde sd = 0$. Comme $\tilde c^2 + \tilde s^2 = 1$, les solutions de l'équation linéaire $\tilde cb + \tilde sd = 0$ sont toutes de la forme $\lambda(-\tilde s, \tilde c), \lambda \in \mathbb R \implies (b, d) = (-\lambda \tilde s, \lambda \tilde c) \implies 1 = \lambda \tilde c^2 + \lambda \tilde s^2 = \lambda(\tilde c^2 + \tilde s^2) \implies \lambda = 1 \implies (b, d) = (-\tilde s, \tilde c)$.
		\item Si $\det M = -1$, on procède de manière similaire et on trouve que $(b, d) = (\tilde s, -\tilde c)$.
	\end{itemize}
	Donc, on a dans tous les cas $(a, c) = (\tilde c, \tilde s)$, et selon le déterminant de $M$, on a soit $(b, d) = (-\tilde s, \tilde c)$, soit $(b, d) = (\tilde s, -\tilde c)$. On a donc bien que
	\begin{itemize}
		\item $M = \begin{pmatrix}
			\tilde c & -\tilde s\\
			\tilde s & \tilde c
		\end{pmatrix}$ si $\det M = 1$, et
		\item $M = \begin{pmatrix}
			\tilde c & -\tilde s\\
			\tilde s & \tilde c
		\end{pmatrix}$ si $\det M = -1$.
	\end{itemize}
\end{proof}

La prochaine proposition, très utile, montre que la matrice de la composée de deux isométries linéaires est le produit des matrices de chacune des isométries.

\begin{prop}
	$\forall \phi, \psi \in \isomo$, avec $M_\phi = \left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}\right)$ et $M_\psi = \left(\begin{smallmatrix} a'&b'\\c'&d' \end{smallmatrix}\right)$. on a
	\begin{equation*}
		M_{\phi \circ \psi} = \begin{pmatrix}
			a&b\\c&d
		\end{pmatrix} \begin{pmatrix}
			a'&b'\\c'&d'
		\end{pmatrix} = \begin{pmatrix}
			aa' + bc' & ab' + bd' \\
			ca' + dc' & cb' + dd'
		\end{pmatrix}
	\end{equation*}
	et $\det M_{\phi \circ \psi} = \det M_\phi \cdot \det M_\psi = (ad-bc)(a'd'-b'c')$.
\end{prop}
\begin{proof}
	$\phi \circ \psi(e_1) = \phi(\psi(e_1)) = \phi((a', c')) = \phi(a'e_1 + c'e_2) = a'\phi(e_1) + c'\phi(e_2) = a'(a, c) + c'(b, d) = (aa' + c'b, a'c + c'd) = (aa' + bc', ca' + dc') =$ la première colonne de $M_{\phi \circ \psi}$. On fait de même avec $\phi \circ \psi (e_2)$ pour trouver la seconde colonne de $M_{\phi \circ \psi}$. \par
	De plus on vérifie que $ \det M_{\phi \circ \psi} = (aa' + bc')(cb' + dd') - (ab' + bd')(ca' + dc') = (ad-bc)(a'd' - b'c') = \det M_\phi \cdot \det M_\psi$.
\end{proof}

\begin{thm}
	\begin{enumerate}
		\item $\isomop$ est un sous-groupe normal de $\isomo$, et de plus il est abélien;
		\item $\isomom$ n'est pas un groupe, mais il est l'ensemble des translatés (à gauche ou à droite) de n'importe quelle isométrie non-spéciale par $\isomo$, càd:
		\begin{equation*}
			 \forall s, s' \in \isomom, \exists r, r' \in \isomop : s' = s \circ r = r' \circ s,
		\end{equation*}
		ou encore:
		\begin{equation*}
			\forall s \in \isomom : \isomom = s \circ \isomop = \isomop \circ s;
		\end{equation*}
		\item Toute isométrie linéaire est la composée d'une ou deux isométries non-spéciales. En particulier $\isomom$ engendre $\isomo$.
	\end{enumerate}
\end{thm}

\begin{proof}[Démonstration du théorème.]
	\begin{enumerate}
		\item On vérifie que $\isomop$ est bien un sous-groupe de $\isomo$:
		\begin{itemize}
			\item $\IdR \in \isomop$;
			\item $\forall r, r' \in \isomop$, il faut montrer que $r \circ r' \in \isomop$, ce qui revient à montrer que $M_{r \circ r'}$ est bien de la forme d'une matrice spéciale:
			\begin{equation*}
				M_{r \circ r'} = \begin{pmatrix}
					c & -s \\
					s & c
				\end{pmatrix} \begin{pmatrix}
					c' & -s' \\
					s' & c'
				\end{pmatrix} = \begin{pmatrix}
					cc'-ss' & -(sc' + cs') \\
					sc' + cs' & cc' - ss'
				\end{pmatrix} = \begin{pmatrix}
					c'' & -s'' \\
					s'' & c''
				\end{pmatrix}.
			\end{equation*}
			De plus, $c''^2 + s''^2 = (cc' - ss')^2 + (sc' + cs')^2 = \ldots = 1$.
			\item Finalement, si $r \in \isomop$, on a $M_r = \left(\begin{smallmatrix} c&-s \\ s&c \end{smallmatrix}\right)$, et si on pose $M_{r^{-1}} = \left(\begin{smallmatrix} c & s \\ -s & c \end{smallmatrix}\right)$, alors
			\begin{equation*}
				M_r \cdot M_{r^{-1}} = M_{r^{-1}} \cdot M_r = M_\IdR,
			\end{equation*}
			donc il existe $r^{-1} \in \isomop$ t.q. $r \circ r^{-1} = r^{-1} \circ r = \IdR$.
			$\isomop$ est donc bien un sous-groupe de $\isomo$. \par
			Il reste à montrer que $\isomop \subgroupnormaleq \isomo$, c.à.d que $\isomop$ est un sous-groupe distingué. Par définition, il faut montrer que $\forall r \in \isomop, \forall \phi \in \isomo, \ad(\phi)(r) \in \isomop$. Cela revient à montrer que $\det M_{\phi \circ r \circ \phi^{-1}} = 1$:
			\begin{align*}
				\det M_{\phi \circ r \circ \phi^{-1}} &= \det M_\phi \cdot \det M_r \cdot \det M_{\phi^{-1}} = \det M_\phi \cdot 1 \cdot \det M_{\phi^{-1}}\\
				&= (\det M_\phi)^2 = (\pm 1)^2\\
				&= 1.
			\end{align*}
			Finalement, on montre que $\isomop$ est un groupe abélien, c.à.d $\forall r, r' \in \isomop, r \circ r' = r' \circ r$. Il suffit de montrer que $M_{r \circ r'} = M_{r' \circ r}$:
			\begin{equation*}
				 M_{r \circ r'} = \begin{pmatrix}
				 	cc' - ss' & -(sc' + cs') \\
				 	sc' + cs' & cc' - ss'
				 \end{pmatrix} = \begin{pmatrix}
				 	c'c - s's & -(s'c + c's) \\
				 	s'c + c's & c'c - s's
				 \end{pmatrix}
				 = M_{r' \circ r}.
			\end{equation*}
		\end{itemize}
		
		\item Soit $s \in \isomom$. On veut montrer que $s \circ \isomop = \{s \circ r \st r \in \isomop \} = \isomom$. On va procéder par double inclusion, en séparant les cas $s \circ \isomop$ et $\isomop \circ s$:
		\begin{itemize}
			\item "$\subseteq$": soit $r \in \isomop$. Alors $\det M_{s \circ r} = \det (M_s \cdot M_r) = \det M_s \cdot \det M_r = (-1) \cdot 1 = -1 \implies s \circ r \in \isomom \implies s \circ \isomop \subseteq \isomom$.
			\item "$\supseteq$": on va maintenant montrer que $s^{-1} \circ \isomo \subseteq \isomop$, ce qui nous permettra d'avoir $s \circ s^{-1} \circ \isomom = \isomom \subseteq s \circ \isomop$ (en composant des deux côtés par $s$). Soit donc $s' \in \isomom$. Alors $\det M_{s^{-1} \circ s'} = \det M_{s^{-1}} \cdot \det M_{s'} = (-1) \cdot (-1) = 1 \implies s^{-1} \circ s' \in \isomop$.
		\end{itemize}
		Donc, $\isomom = s \circ \isomop$. On fait de même pour montrer $\isomom = \isomop \circ s$.
		
		\item On veut montrer que toute isométrie linéaire est la composée d'une ou deux isométries non-spéciales.
		\begin{itemize}
			\item Si $\phi \in \isomom, \phi = s$ et $\phi$ est déjà "la composée d'une isométrie non-spéciale".
			\item Si $\phi \in \isomop$, $\forall s \in \isomom$ on a $\det M_{\phi \circ s^{-1}} = 1 \cdot (-1) = -1 \implies \phi \circ s^{-1} = s' \in \isomom \implies \phi = s' \circ s$, et donc $\phi$ est bien la composée de deux isométries non-spéciales.
		\end{itemize}
	\end{enumerate}
\end{proof}

Pfiou ! Une page pour un théorème en 3 parties qui ne sont pas apparemment directement liées, ça n'est pas très digeste. Peut-être briserai-je ce théorème en plusieurs parties abordables dans une prochaine édition ! \par
Ceci dit, on avance déjà vers le prochain théorème:

\begin{thm}
	\begin{enumerate}
		\item Soit $s \in \isomom$. Alors $s^2 = \IdR$ (et $s \neq \IdR$).	
		\item Soient $r \in \isomop$ et $\phi \in \isomo$. Alors 
		\begin{itemize}
			\item soit $\phi \in \isomom$ et $\ad(\phi)(r) = r^{-1}$,
			\item soit $\phi \in \isomop$ et $\ad(\phi)(r) = r$.
		\end{itemize}
	\end{enumerate}
\end{thm}
\begin{proof}
	\begin{enumerate}
	 	\item Soit $s \in \isomom$ ($s \neq \IdR$ car $\IdR \notin \isomom$). Alors
	 	\begin{equation*}
	 		M_{s \circ s} = \begin{pmatrix}
	 			c&s\\s&-c
	 		\end{pmatrix} = \begin{pmatrix}
	 			c&s\\s&-c
	 		\end{pmatrix} = \begin{pmatrix}
	 			c^2 + s^2 & cs - sc \\
	 			sc - cs & s^2 + (-c)^2
	 		\end{pmatrix} = \begin{pmatrix}
	 			1&0\\0&1
	 		\end{pmatrix}
	 		= M_\IdR.
	 	\end{equation*}
	 	\item Deux cas sont possibles:
	 	\begin{itemize}
	 		\item si $\phi = s \in \isomom$, pour montrer que $s \circ r \circ s^{-1} = r^{-1}$, il suffit de montrer que $(s \circ r \circ s^{-1}) \circ r = \IdR$. Or $s \circ r \circ s^{-1} \circ r = s \circ r \circ s \circ r = s' \circ s' = \IdR$ (car $s \circ r \in \isomom$, comme vu précédemment).
	 		\item sinon, $\phi = r' \in \isomop$ et $\ad(r')(r) = r' \circ r \circ r'^{-1} = r' \circ r'^{-1} \circ r = r$.
	 	\end{itemize}
	\end{enumerate}
\end{proof}

\section{La géométrie des isométries linéaires}
Après avoir exploré certaines des propriétés des matrices associées aux isométries linéaires, nous pouvons prendre un peu de recul et nous souvenir que nos considérations étaient, originellement, géométriques. Nous allons donc tenter de saisir mathématiquement cet aspect central. Une manière de le faire est l'étude des points fixes de ces transformations.

\begin{defn}
	Un \textbf{point fixe} d'une application $\phi : X \to X$ est un élément $x$ tel que $\phi(x) = x$.
\end{defn}

\begin{defn}
	On nomme $\Fix(\phi)$ l'ensemble de tous les points fixes de l'application $\phi : X \to X$.
\end{defn}

C'est surtout cette dernière définition qui nous intéressera: en effet, si une isométrie linéaire a pour ensemble de points fixes un unique point, ou une droite (ou même l'ensemble du plan), cela donnera lieu à de nouvelles définitions mathématiques pour (respectement) les rotations et les symétries linéaires (qui sont les deux types d'isométries linéaires que l'on pense intuitivement exister).

\begin{prop}
	Soit $\phi \in \isomo$. Alors $\Fix(\phi)$ est un sous-espace vectoriel de $\plan$.
\end{prop}
\begin{proof}
	Soit $\vec v \in \plan$. Alors $\vec v \in \Fix(\phi) \iff \phi(\vec v) - \vec v = 0 \iff (\phi-\IdR)(\vec v) = \origin \iff \vec v \in \ker(\phi - \IdR)$. Or $\phi - \IdR$ est une application linéaire (car $\phi$ et $\IdR$ sont des applications linéaires), et donc, par un résultat d'algèbre linéaire, $\ker(\phi - \IdR) = \Fix(\phi)$ est un espace vectoriel.
\end{proof}

La proposition suivante cherche à confirmer les intuitions que l'on avait quand aux natures possibles des isométries linéaires:
\begin{prop}
	Soit $\phi \in \isomo$. Alors un et un seul des cas suivants se présente:
	\begin{itemize}
		\item $\phi = \IdR$ et $\Fix(\phi) = \plan$
		\item $\phi \in \isomop \setminus \{\IdR\}$ et $\Fix(\phi) = \{\origin\}$
		\item $\phi \in \isomom$ et $\Fix(\phi) = \mathbb R \vec u = \{\lambda \vec u \st \lambda \in \mathbb R\}$, une droite linéaire de $\plan$
	\end{itemize}
\end{prop}
\begin{proof}
	\begin{itemize}
		\item Si $\phi = \IdR$, on a bien $\Fix(\phi) = \plan$.
		\item Si $\phi \in \isomop \setminus \{\IdR\}$, alors si $(x, y) \in \Fix(\phi)$, on a $(x, y) = \phi((x, y)) = x\phi(e_1) + y\phi(e_2) = x(\tilde c, \tilde s) + y(-\tilde s, \tilde c) = (\tilde cx - \tilde sy, \tilde sx + \tilde cy)$, ce qui donne le système d'équations linéaires
		\begin{equation*}
			\begin{cases}
				\tilde cx - \tilde sy = x\\
				\tilde sx + \tilde cy = y
			\end{cases} \iff
			\begin{cases}
				(\tilde c - 1)x - \tilde sy = 0\\
				\tilde sx + (\tilde c - 1)y = 0
			\end{cases}.
		\end{equation*}
		Le déterminant de ce système est $(\tilde c - 1)^2 + s^2$. Comme $\phi \neq \IdR$, on a $(\tilde c, \tilde s) \neq (1, 0)$. Donc ce déterminant n'est pas égal à 0, et il existe une unique solution (par un résultat d'algèbre linéaire). En résolvant le système algébriquement, on trouve que cette solution est $\origin$.
		\item Si $\phi \in \isomom$, alors si $(x, y) \in \Fix(\phi)$, on a $(x, y) = \phi((x, y)) = x\phi(e_1) + y\phi(e_2) = x(\tilde c, \tilde s) + y(\tilde s, -\tilde c) = (\tilde cx + \tilde sy, \tilde sx - \tilde cy)$, ce qui donne le système d'équations linéaires
		\begin{equation*}
			\begin{cases}				
				\tilde cx + \tilde sy = x\\
				\tilde sx - \tilde cy = y
			\end{cases} \iff
			\begin{cases}
				(\tilde c - 1)x + \tilde sy = 0\\
				\tilde sx - (\tilde c + 1)y = 0
			\end{cases}.
		\end{equation*}
		Ici, le déterminant vaut $-(c-1)(c+1) - s^2 = 1 - (c^2 + s^2) = 1-1 = 0$, et les deux lignes du système sont donc linéairement dépendantes. L'ensemble des points fixes satisfait donc
		\begin{equation*}
			(c-1)x + sy = 0
		\end{equation*}
		qui est l'équation d'une droite linéaire de $\plan$. Dans le cas où $(\tilde c, \tilde s) = (1, 0)$, cette équation est triviale ($0 = 0$) et on regarde l'autre équation du système, qui donne
		\begin{equation*}
			-2y = 0
		\end{equation*}
		c'est-à-dire la droite des abscisses.
	\end{itemize}
\end{proof}

Bien! Pas de surprise, nous nous attendions intuitivement à ne trouver que ces deux types principaux d'isométrie.

\subsection{Les isométries linéaires non-spéciales, ou symétries linéaires}

\begin{thm}
	Soit $\phi \in \isomop$ avec $\Fix(\phi) = \mathbb R \vec u$. Alors $\phi$ est donnée par 
	\begin{equation}
		\phi(\vec w) = \vec w - 2\frac{\scalaire{\vec w, \vec v}}{\longueur{v}^2} \vec v,
	\end{equation}
	où $\vec v$ est un vecteur perpendiculaire à $\vec u$.\\
	Cas spéciaux:
	\begin{itemize}
		\item $\phi(\lambda \vec u) = \lambda \vec u$ par définition, et
		\item $\phi(\lambda \vec v) = -\lambda \vec v$.
	\end{itemize}
\end{thm}
\begin{proof}
	Soit $\phi \in \isomom$. Alors $\Fix(\phi) = \mathbb R \vec u, \vec u \neq \origin$. Il existe $\vec v \neq \origin$ tel que $\vec v \perp \vec u$: si $\vec u = (a, b)$, alors $\vec v = \lambda(-b, a)$ est tel que $\scalaire{\vec v, \vec u} = 0$. Comme $\vec u$ et $\vec v$ sont alors perpendiculaires, ils forment une base orthogonale de $\plan$, et comme $\phi$ est linéaire, on peut entièrement déterminer $\phi$ par l'image de cette base. Or, $\phi(\vec u) = \vec u$, comme montré précédemment, et il nous reste à calculer $\phi(\vec v)$. On a
	\begin{equation*}
		\phi(\vec v) = \frac{\scalaire{\phi(\vec v), \vec u}}{\scalaire{\vec u, \vec u}}\vec u + \frac{\scalaire{\phi(\vec v), \vec v}}{\scalaire{\vec v, \vec v}}\vec v,
	\end{equation*}
	il nout faut donc trouver les expressions de $\scalaire{\phi(\vec v), \vec u}$ et $\scalaire{\phi(\vec v), \vec v}$:
	\begin{align*}
		&\scalaire{\phi(\vec v), \vec u} = \scalaire{\phi(\vec v), \phi(\vec u)} = \scalaire{\vec v, \vec u} = 0
	\end{align*}
	donc $\phi(\vec v)$ est perpendiculaire à $\vec u$ et est donc colinéaire à $\vec v$, c.à.d. $\phi(\vec v) = \mu \vec v$. Or, comme $\phi$ est une isométrie, on a
	\begin{align*}
		\scalaire{\vec v, \vec v} &= \scalaire{\phi(\vec v), \phi(\vec v)}\\
		&= \scalaire{\mu \vec v, \mu \vec v}\\
		&= \mu^2 \scalaire{\vec v, \vec v}, \text{ç.à.d.}\\
		\scalaire{\vec v, \vec v} &= \mu^2 \scalaire{\vec v, \vec v}\\
		\iff \mu^2 &= 1 \iff \mu = \pm 1.
	\end{align*}
	Or, si $\mu = 1$, on a $\phi(\vec u) = \vec u$ et $\phi(\vec v) = \vec v$ et donc $\phi = \IdR$, ce qui est impossible car $\IdR \notin \isomom$. Donc, $\mu = -1$ et $\phi(\vec v) = -\vec v$.\par
	Soit maintenant $\vec w \in \plan$ quelconque. Pour calculer $\phi(\vec w)$, on exprime $\vec w$ dans la base $(\vec u, \vec v)$:
	\begin{align*}
		\vec w &= \frac{\scalaire{\vec w, \vec u}}{\scalaire{\vec u, \vec u}} \vec u + \frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}} \vec v,
	\intertext{et donc}		
		\phi(\vec w) &= \frac{\scalaire{\vec w, \vec u}}{\scalaire{\vec u, \vec u}} \phi(\vec u) + \frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}} \phi(\vec v)\\
		&= \frac{\scalaire{\vec w, \vec u}}{\scalaire{\vec u, \vec u}}\vec u - \frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}} \vec v\\
		&= \underbrace{\frac{\scalaire{\vec w, \vec u}}{\scalaire{\vec u, \vec u}}\vec u + \frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}} \vec v}_{\vec w}
		- \frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}} \vec v - \frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}} \vec v\\
		&= \vec w - 2\frac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}} \vec v
	\end{align*}
\end{proof}

Maintenant qu'on les connaît assez bien, il est temps de donner officiellement à ces isométries le nom qu'elles méritent!

\begin{defn}
	On appelle $\isomom$ l'ensemble des \textbf{symétries linéaires} du plan. Si $\phi \in \isomom$, on dit que $\phi$ est une \textbf{symétrie linéaire}.
\end{defn}

\begin{thm}[Matrice d'une symétrie]
	Soit $s \in \isomom$ et $M_s = \left(\begin{smallmatrix} c&s\\s&-c \end{smallmatrix}\right)$, avec $c^2 + s^2 = 1$. Alors les vecteurs $\vec u$ et $\vec v$ qui décrivent $s$ géométriquement sont donnés par
	\begin{equation*}
		\begin{cases}
			\vec u = (1, 0) \text{ et } \vec v = (0, 1) \text{ si } c = 1\\
			\vec u = (0, 1) \text{ et } \vec v = (1, 0) \text{ si } c = -1\\
			\vec u = (-s, -(c-1)) \text{ et } \vec v = (s, -(c+1)) \text{ en général.}
		\end{cases}
	\end{equation*}
\end{thm}
\begin{proof}
	Vérifier $\vec u, \vec v \neq \origin, \vec u \perp \vec v, s(\vec u) = \vec u$ et $s(\vec v) = -\vec v$.
\end{proof}
\begin{remark}
	$\vec u$ et $\vec v$ ne sont pas uniques: si $(\vec u, \vec v)$ marche alors $(\lambda \vec u, \mu \vec v)$ marche aussi $\forall \lambda, \mu \in \mathbb R$.\par
	Pour trouver la matrice d'une symétrie à partir de la formule $\phi(\vec w) = \vec w - \dfrac{\scalaire{\vec w, \vec v}}{\scalaire{\vec v, \vec v}}$, on suppose $\vec u = (a, b)$ ($\implies \vec v = (-b, a)$) et on calcule $\phi(\vec e_1)$ ou $\phi(\vec e_2)$. CLARIFIER ???????????????????????????????????????????????????????????????????????????????
\end{remark}

\subsection{Les isométries linéaires spéciales, ou rotations linéaires}
Passons maintenant à l'autre type d'isométries linéaires : les isométries spéciales. Deux cas se présentent:
\begin{itemize}
	\item soit $r = \IdR$, et on dit que $r$ est la rotation triviale
	\item soit $r \neq \IdR$, et on dit que $r$ est une rotation autour de l'origine.
\end{itemize}

\begin{prop}\label{bij-isomop-unitcircle}
	Soit $\unitcircle = \{(c, s) \in \plan \st c^2 + s^2 = 1\}$ le cercle unité de $\plan$. Alors il existe une bijection entre $\isomop$ et $\unitcircle$.
\end{prop}
\begin{proof}
	Cette bijection est donnée par
	\begin{align*}
		r \in \isomop \mapsto M_r = \begin{pmatrix}
		c & -s\\
		s & c
		\end{pmatrix}
		\mapsto (c, s) \in \unitcircle.
	\end{align*}
\end{proof}

\begin{thm}
	Soient $P, Q \in \unitcircle$. Alors il existe exactement une rotation $r \in \isomop$ telle que $r(P) = Q$. En particulier, la réciproque de la bijection de la proposition \ref{bij-isomop-unitcircle} est donnée en associant à $Q \in \unitcircle$ l'unique rotation $r$ telle que $r(1, 0) = Q$.
\end{thm}
\begin{proof}
	Soient $P = (c, s), Q = (c', s') \in \unitcircle$. On commence par montrer l'existence de $r$ telle que $r(P) = Q$. On pose
	\begin{itemize}
		\item $r_P$ la rotation de matrice $\begin{pmatrix}
			c & -s\\
			s & p
		\end{pmatrix}$. Alors $r_P((1, 0)) = (c, s) = P$.
		\item $r_Q$ la rotation de matrice $\begin{pmatrix}
			c' & -s'\\
			s' & p'
		\end{pmatrix}$. Alors $r_P((1, 0)) = (c', s') = P$.
	\end{itemize}
	On peut montrer facilement (!) que $r_Q \circ r_P^{-1}$ envoie $P$ sur $Q$.\par
	On montre maintenant l'unicité de cette rotation. Soient $r$ et $r'$ telles que $r(P) = r'(P) = Q$. Alors $r'^{-1} \circ r = r''$ est une rotation et $r'^{-1} \circ r(P) = r'^{-1}(Q) = P$, donc $P \in \Fix(r'')$ et $P \neq \origin$, ce qui implique que $r'' = \IdR$ et donc $r = r'$.
\end{proof}

Ce résultat donne sens à la définition suivante de l'angle.

\begin{defn}
	Soient $P, Q \in \unitcircle$. On définit l'\textbf{angle $\widehat{POQ}$} comme étant l'unique rotation $r_{PQ} \in \isomop$ qui envoie $P$ sur $Q$. Si la matrice de $r_{PQ}$ dans la base canonique est $M_{r_{PQ}} = \left(\begin{smallmatrix} c & -s \\ s & c \end{smallmatrix}\right)$, alors on définit le \textbf{cosinus} de l'angle $\widehat{POQ}$ comme étant le nombre $c$, et son \textbf{sinus} comme étant le nombre $s$.\par
	De plus, on définit la somme de deux angles $r$ et $r'$ comme étant l'angle $r \circ r' = r' \circ r$.
\end{defn}

On peut faire quelques observations et remarques sur la généralisation de cette définition de l'angle à d'autres objets de $\plan$:
\begin{itemize}
	\item soient $\hat u$ et $\hat v$ des vecteurs unitaires, avec $\hat u = \overrightarrow{OP}$ et $\hat v = \overrightarrow{OQ}$. Alors l'angle $\widehat{\hat u \hat v}$ est l'unique rotation qui envoie $\hat u$ sur $\hat v$.
	\item soient $\vec u$ et $\vec v$ des vecteurs non-nuls. L'angle $\widehat{\vec u, \vec v}$ est l'angle $\widehat{\hat u \hat v}$ où $\hat u$ (resp. $\hat v$) sont les vecteurs $\vec u$ (resp. $\vec v$) normalisés, c'est-à-dire divisés par leur norme.
	\item l'angle entre deux segments orientés non-triviaux $[PQ]$ et $[P'Q']$ est l'angle $\widehat{\overrightarrow{PQ} \overrightarrow{P'Q'}}$ (notation très moche).
	\item l'angle entre deux droites linéaires $D_0 = \mathbb R \vec u$ et $D_0' = \mathbb R \vec v$ est \textbf{l'ensemble des deux rotations} qui envoient $D_0$ sur $D_0'$, c'est-à-dire les deux angles $\widehat{\vec u \vec v}$ et $\widehat{\vec u (-\vec v)}$.
\end{itemize}

\begin{exmp}
	On peut citer quelques angles remarquables:
	\begin{equation*}\begin{array}{m | m | m}
	
	\end{array}
\end{exmp}

\end{document}