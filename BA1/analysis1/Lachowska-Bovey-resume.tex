\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\newcommand\eqdef{\; \stackrel{\mathclap{\normalfont\text{déf.}}}{ = } \;} % for definitions
\newcommand\eqbl{\; \stackrel{\mathclap{\normalfont\text{BL}}}{ = } \;} % for applications of Bernouilli-l'Hospital's theorem
\numberwithin{equation}{subsection}

\usepackage{array}
\newcolumntype{L}{>{$}l<{$}} %math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}} %math-mode version of "c" ctolumn type
%the above column types were found on https://tex.stackexchange.com/a/112585

\usepackage{hyperref}

%margins
\addtolength{\oddsidemargin}{-2.5cm}
\addtolength{\evensidemargin}{-2.5cm}
\addtolength{\textwidth}{4.5cm}
\addtolength{\topmargin}{-2.5cm}
\addtolength{\textheight}{4.5cm}

\title{\vspace{-1.5cm} Analyse 1 -  Anna Lachowska \\ Résumé}
\author{Benjamin Bovey - EPFL IC}
\date{November 2018}

\begin{document}

\maketitle

\section*{Introduction}
Ce document est destiné à résumer les certainement laborieux cours d'Analyse 1 présentés par Mme Lachowska, afin d'en présenter uniquement les aspects les plus cruciaux quand à la résolution d'exercices. Il fait partie d'un projet auquel vous pouvez participer! Plus d'informations sur \href{https://github.com/Arakniode/almighty-handbook-of-sleep-deprived-student}{le GitHub du projet} (\url{https://github.com/Arakniode/almighty-handbook-of-sleep-deprived-student}). \\
Ce résumé est pour l'instant incomplet par rapport à l'ensemble des notions couvertes par le cours d'analyse 1, ce cours n'ayant pas encore touché à sa fin. 

\section{Identités}
Ces quelques identités de base permettent de transformer des expressions, ce qui est souvent utile lorsqu'on cherche leur limite et que celle de l'équation de base est indéterminée. Il faut les connaître, mais surtout savoir les \emph{reconnaître} et les dénicher. N'oublions pas que les rédacteurs d'exercices aiment bien cacher les solutions sous ce genre d'identités simples, mais parfois bien camouflées.
\subsection{Identités algébriques}
\subsubsection{Polynômes} 
\(x,y \in \mathbb{R}\):
\begin{align*}
	&(x+y)^2 = x^2 + 2xy + y^2 				& 	&x^2 - y^2 = (x - y)(x + y) \\
	&(x-y)^2 = x^2 - 2xy + y^2 					& 	&x^3 - y^3 = (x - y)(x^2 + xy + y^2) \\
	&(x+y)^3 = x^3 + 3x^2y + 3xy^2 + y^3 	& 	&x^3 + y^3 = (x + y)(x^2 - xy + y^2) \\
	&(x-y)^3 = x^3 - 3x^2y + 3xy^2 - y^3 
\end{align*}
\emph{N.B}: Il est utile de se souvenir que \(1^2 = 1^3 = 1\), ce qui peut aider à dénicher des expressions de la forme \(x^3 \pm y^3\). Par exemple:
\vspace{-0.2cm}
\begin{align*}
	1 - a^3 	&= 1^3 - a^3 \\
				&= (1-a)(1+a+a^2)
\end{align*}
Dans le cas, par exemple, d'une limite fraction de polynômes, dans lequel l'expression de base donnerait une limite indéterminée, on pourrait peut-être utiliser cette identité pour simplifier le calcul de la limite, en faisant apparaître des facteurs communs au numérateur et au dénominateur.

\subsubsection{Exponentielles}
\(a, b \in \mathbb{R}_{>0}, \; x, y \in \mathbb{R}\)
\begin{align*}
	a^x \cdot a^y 		&= a^{x+y} 			&	(a^x)^y 							&= a^{x \cdot y} \\
	\dfrac{a^x}{a^y} 	&= a^{x-y}  			&	\sqrt[n]{a}						&= a^{\frac{1}{n}}, \quad n \in \mathbb{N}_{>0} \\
	(ab)^x 				&= a^x \cdot b^x 	&	\left (\dfrac{a}{b} \right )^x 	&= \dfrac{a^x}{b^x} \\
	a^0 					&= 1 					& 	a^1 								&= a
\end{align*}

\subsubsection{Logarithmes}
On assume que la notation \(\log\) sans indice précisé dénote le logarithme naturel, car c'est la convention utilisée par Mme Lachowska dans son cours. \\
\(a, b \in \mathbb{R}_{>0}, \; c \in \mathbb{R}\)
\begin{align*}
	&\log(ab) = \log(a) + \log(b) 
		& &\log{\left (\dfrac{a}{b}\right )} = \log (a) - \log (b) 
			& &\log(a^c) = c \cdot \log(a) \\
	&\log_a(1) = 0 
		& &\log(e) = 1 
			& &\log_a(a)	= 1, \; a \neq 1 \\
	&\log_a(b) = \frac{\log(b)}{\log(a)}
\end{align*}
L'identité de changement de base est applicable avec toute autre base réelle, pas juste base \(e\).

\subsubsection{Trigonométrie}
\emph{N.B: quelques identités des fonctions trigonométriques peuvent être retrouvées dans les \hyperref[sec:hypertrigo]{annexes}.}
\begin{align}
	\label{eq:anglesum}
	&\sin(x \pm y) 	= \sin(x) \cos(y) \pm \cos(x) \sin(y)	& &\cos(x \pm y) 	= \cos(x) \cos(y) \mp \sin(x)\sin(y) \\
	\label{eq:doubleangle}
	&\begin{cases}
		\begin{aligned}	
			\sin(2x) = 2\cos(x)\sin(x)
		\end{aligned} 
	\end{cases} & &\begin{cases}
		\begin{aligned}
			\cos(2x) 	&= 1 - 2\sin^2(x) \\
						&= -1 + 2\cos^2(x)  \\
						&= \cos^2(x) - \sin^2(x)
		\end{aligned}
	\end{cases} \\
	&\cos^2(x) + \sin^2(x) = 1 \\
	&\tan(x) = \frac{\sin(x)}{\cos(x)} \\
	&\sin(-x) = -\sin(x) \quad \text{(impaire)}		& &\cos(-x) = \cos(x) \quad \text{(paire)}
\end{align}
Souvenons-nous que l'on peut parfois "compliquer nos équations pour les simplifier": par exemple, l'identité \(\cos^2(x) + \sin^2(x) = 1\) peut être retrouvée à partir de l'identité \ref{eq:anglesum} par les observations suivantes: 
\vspace{-0.2cm}
\begin{align*}
	\cos(0) 	&= 1 \quad \text{(\emph{se référer au tableau des valeurs de \(\cos(x)\) et \(\sin(x)\) ci-dessous})}
\end{align*}
\vspace{-0.2cm}
mais aussi: 
\begin{align*}
	\cos(0)	&= \cos(x-x) \\
				&= \cos^2(x) + \sin^2(x) 
\end{align*}
Donc, \(\cos^2(x) + \sin^2(x) = 1\). \\
Il est toujours plus simple de trouver une limite lorsque le facteur de \(x\) dans \(\sin(x)\) ou \(cos(x)\) est simplement \(1\). Les identités \ref{eq:doubleangle}, appelées "\emph{identités de l'angle double}", sont donc très utiles dans la recherche de limite. \\
Il n'y a malheureusement pas d'autre façon que de s'entraîner par des exercices afin d'apprendre à connaître quand et comment utiliser ces identités. Cependant, dans les exercices de recherche de limites comportant des expressions, par exemple, du type \(\sin(3x)\), on peut généralement assumer qu'il faudra transformer l'expression en \(\sin(2x + x)\), puis utiliser les identités \ref{eq:anglesum} et \ref{eq:doubleangle} pour simplifier.

\subsubsection{Quelques valeurs de \(\cos(x)\) et \(\sin(x)\)}
\begin{center}
	\def\arraystretch{1.5}
	\begin{tabular}{| C | C | C | C |} %https://tex.stackexchange.com/questions/112576/math-mode-in-tabular-without-having-to-use-everywhere
		\hline
		x 						& \sin(x) 							& \cos(x)						& \tan(x) \\ \hline
		0 						& 0								& 1							& 0\\
		\frac{\pi}{6}			& \frac{1}{2} 					& \frac{\sqrt{3}}{2}			& \frac{\sqrt{3}}{3} \\
		\frac{\pi}{4}			& \frac{\sqrt{2}}{2} 			& \frac{\sqrt{2}}{2}			& 1 \\
		\frac{\pi}{3}			& \frac{\sqrt{3}}{2} 			& \frac{1}{2}					& \sqrt{3} \\
		\frac{\pi}{2}			& 1								& 0							& \infty \\
		\hline
	\end{tabular}
\end{center}
Pour se souvenir de ces valeurs importantes, on peut observer que les valeurs de \(\sin(x)\) croissent avec l'angle, et décroissent pour \(\cos(x)\) (penser aux graphes des fonctions!). Il existe une symmétrie verticale entre la colonne \(\sin(x)\) et la colonne \(\cos(x)\).

\section{Limites utiles}
Ces limites ont été définies lors du cours, parfois dans le cadre des suites, et parfois dans celui des fonctions. Généralement, les limites de suites et les séries sont exprimées par rapport à \(n\), et les limites de fonctions par rapport à \(x\). Elles s'appliquent cependant dans la majorité des cas, et selon le bon sens, de la même façon dans les deux cas.

\begingroup\allowdisplaybreaks[1] %to avoid all the equations creating page jumps everywhere, we need to allow them to separate within the align environment
\begin{flalign}
		&\nonumber\text{Soient \(P_n\) et \(Q_n\) deux suites polynomiales:} \\
	&\lim_{n\to\infty} \dfrac{P_n}{Q_n} = 
		\begin{cases}
			0, 						&\text{si } \deg{P_n} < \deg{Q_n} \\
			\frac{p_n}{q_n},	&\text{si } \deg{P_n} = \deg{Q_n}, \; \text{avec \(p_n\) et \(q_n\) les coefficients du terme de plus haut degré}\\
			\infty , 				&\text{si } \deg{P_n} > \deg{Q_n}
		\end{cases} \\
	&\lim_{n\to\infty} \dfrac{1}{n^p} = 0 \quad \forall p \in \mathbb{R}_+^* \\
	&\lim_{n\to\infty} \sqrt[n]{a} = 1 \quad \forall a \in \mathbb{R}_+ \\
	&\lim_{n\to\infty} \dfrac{p^n}{n!} = 0 \quad \forall p \in \mathbb{R}_+^* \\
	&\lim_{n\to\infty} \frac{\sin{\frac{1}{n}}} {\frac{1}{n}} = 1 \\
	&\lim_{n\to\infty} \sin{\frac{1}{n}} = 0 \\
	&\lim_{n\to\infty} \left (1 + \frac{1}{n} \right )^n = e  \\
	&\lim_{n\to\infty} \left (1 - \frac{1}{n} \right )^n = \frac{1}{e} = e^{-1} \\
	&\lim_{n\to\infty} \frac{n!}{n^n} = 0 \\
	&\sum_{k=0}^{\infty} r^k = \begin{cases}
										\dfrac{1}{1-r}, &|r| < 1 \\
										\text{diverge, } &|r| \geq 1
									   	\end{cases}\quad r \in \mathbb{R} \\
	&\sum_{n=1}^{\infty} \frac{1}{n^p} \text{ converge } \forall p \in \mathbb{R}_{>1} \\
	&\sum_{n=1}^{\infty} \frac{1}{n} \text{ diverge. } \\
	&\sum_{n=0}^{\infty} |a_n| \text{ converge } \Rightarrow \sum_{k=0}^{\infty} a_n \text{ converge. } ( \nLeftarrow )\\
	&\lim_{x \to 0} \dfrac{\sin{x}}{x} = 1 \\
	&\lim_{x \to 0} \sin{\frac{1}{x}} \quad \text{n'existe pas.} \\
	&\lim_{x \to 0} x \cdot \sin{\frac{1}{x}} = \lim_{x \to 0} x \cdot \cos{\frac{1}{x}} = 0 \\
	&e^x \eqdef \sum_{n=0}^{\infty} \dfrac{x^n}{n!} \\
	&\lim_{x \to 0} \dfrac{e^x-1}{x} = 1 \\
	&\lim_{x \to \infty} \frac{\log(x)}{x^\alpha} = 0, \; \alpha > 0
\end{flalign}
\allowdisplaybreaks[0]\endgroup

\subsection{Critères de convergence}
Ces critères permettent de déterminer si une \textbf{série} de la forme \(\sum^\infty a_n\) converge ou non. Souvenons-nous que \(\lim_{x \to 0} a_n = 0\) est une \emph{condition nécessaire} à la convergence, mais n'implique pas la convergence. \\

\subsubsection{Critère d'Alembert}
\begin{equation*}
	\lim_{n \to \infty} \left | \dfrac{a_{n+1}}{a_n} \right | = \rho \in \mathbb{R} 
	\begin{cases}
		\rho < 1 \Rightarrow \sum_{n=0}^\infty a_n \ \text{converge absolument.} \\
		\rho > 1 \Rightarrow \sum_{n=0}^\infty a_n \ \text{diverge.} \\
		\rho = 1 \quad \text{ne nous indique rien sur la convergence de la série.}
	\end{cases}
\end{equation*}

\subsubsection{Critère de Cauchy}
\begin{equation*}
	\lim_{n \to \infty} \left | a_n \right |^{\frac{1}{n}} = \rho \in \mathbb{R}
	\begin{cases}
		\rho < 1 \Rightarrow \sum_{n=0}^\infty a_n \ \text{converge absolument.} \\
		\rho > 1 \Rightarrow \sum_{n=0}^\infty a_n \ \text{diverge.} \\
		\rho = 1 \quad \text{ne nous indique rien sur la convergence de la série.}
	\end{cases}
\end{equation*}

\subsubsection{Critère de comparaison}
Soient \(a_n\) et \(b_n\) deux suites telles que \(\exists k : \forall n \geq k b_n \geq a_n\):
\begin{equation*}
	\begin{cases}
		\sum_{n=0}^\infty b_n \text{converge} \Rightarrow \sum_{n=0}^\infty a_n \ \text{converge} \\
		\sum_{n=0}^\infty a_n \text{diverge} \Rightarrow \sum_{n=0}^\infty b_n \ \text{diverge}
	\end{cases}
\end{equation*}

\subsubsection{Critère de Leibniz}
\begin{equation*}
	\begin{rcases} %rcases environment from the mathtools package, bracket to the right of cases
		\exists p \in \mathbb{N} : \forall n \geq p \quad |a_{n+1}| \leq |a_n| \\
		\exists p \in \mathbb{N} : \forall n \geq p \quad a_{n+1} \cdot a_n \leq 0 \\
		\lim_{n \to \infty} a_n = 0
	\end{rcases}
	\Rightarrow \sum_{n=0}^\infty a_n \: \text{converge.}
\end{equation*}
La seconde condition peut sembler étrange, mais il s'agit juste de la définition formelle de l'alternance: comme chaque terme positif est suivi d'un terme négatif et vice-versa, si on multiplie deux termes arbitraires consécutifs, le résultat devrait être négatif.

\subsubsection{Conclusion sur les critères de convergence}
Il est généralement optimal d'appliquer ces critères dans cet ordre précis (sauf si la suite est alternée, auquel cas on préférera Leibniz) pour plusieurs raisons:
\begin{itemize}
	\item ils sont classés par ordre de difficulté d'application (encore une fois, la difficulté dépend de la suite et certaines suites appellent à l'utilisation de certains critères). %TODO: note de bas de page pour exemple suite puissance n?
	\item Le critère d'Alembert est souvent plus facile à appliquer, cependant il existe des cas où le critère d'Alembert ne donne aucun information (\(\rho = 1\)), mais où le critère de Cauchy nous en donne. Le cas inverse n'existe pas.
\end{itemize}
 
\section{Formes indéterminées}
Voici toutes les formes indéterminées qui peuvent être rencontrées lors du calcul de limite:
\begin{align*}
	&\infty - \infty, \quad \dfrac{\infty}{\infty}, \quad \dfrac{0}{0}, \quad 0 \cdot \infty, \quad 0^0, \quad \infty^0, \quad 1^\infty
\end{align*}
Pour les valeurs des limites de puissances avec \(0, 1, \text{ ou } \infty\) en base ou en exposant, il y a trois cas d'indétermination. Ces cas sont soulignés ci-dessous. \\ Dans les cas déterminés, la valeur de la limite sera la valeur de la base.
\begin{align*}
	&\underline{0^0	}		& &0^1					& &0^\infty \\
	&\underline{\infty^0}	& &\infty^1				& &\infty^\infty \\
	&1^0						& &1^1					& &\underline{1^\infty}
\end{align*}

\section{Dérivées}

\subsection{Définition}
La dérivée d'une fonction \(f(x)\) en \(x=x_0\) est définie par la limite suivante:
\begin{equation*}
	f'(x_0) = \lim_{x \to x_0} \dfrac{f(x) - f(x_0)}{x - x_0}
\end{equation*}
On peut également utiliser cette définition, qui est équivalente:
\begin{equation*}
	f'(x_0) = \lim_{h \to 0} \dfrac{f(x_0 + h) - f(x_0)}{h}
\end{equation*}
Cette seconde définition est peut-être plus facile à lier à une représentation graphique de la dérivée: on prend deux points sur la fonction, \(x_0\) et \(x_0 + h\), on regarde la différence verticale entre les deux \(\left (f(x_0 + h) - f(x_0)\right )\) et on la divise par la différence horizontale entre les deux \(\left (\frac{f(x_0 + h) - f(x_0)}{h}\right )\). Ici, nous avons retrouvé la formule de la pente d'un graphe linéaire, ou de l'estimation de la pente d'un graphe courbe. Tout ce que la dérivée ajoute à la formule, c'est une limite afin d'améliorer la précision de cette estimation. On accomplit cela en "pinçant" les deux points ensemble, c'est à dire en réduisant la distance \(h\) qui les sépare en la faisant tendre vers 0. On se retrouve donc non plus avec une estimation, mais avec la valeur exacte de la pente du graphe en ce point.

\subsection{Dérivées utiles}
\begin{align*}
	&(\sin(x))' = \cos(x)
		& &(\cos(x))' = -\sin(x)
			& &(\tan(x))' = \frac{1}{\cos^2(x)} \\
	&(x^2)' = 2x
		& &(x^3)' = 3x^2
			& &(x^n)' = n \cdot x^{n-1} \\
	&(e^x)' = e^x
		& &(\log(x))' = \frac{1}{x}
\end{align*}

\subsection{Opérations sur les dérivées}
Soient les applications \(f: \mathbb{R} \to \mathbb{R}\) et \(g: \mathbb{R} \to \mathbb{R}\) et les réels \(\alpha, \beta \in \mathbb{R}\):
\begin{align*}
	&(\alpha f + \beta g)' = \alpha f' + \beta g' 
		& &(f \cdot g)' = f' \cdot g + f \cdot g' 
			& &\left (\frac{f}{g}\right ) = \frac{f' \cdot g - f \cdot g'}{g^2} \\ 
	&(f \circ g)'(x) = g'(f(x)) \cdot f'(x) \\
	&(f^{-1})' (f(x_0))  = \frac{1}{f'(x_0)} %should I really include this (used only for proofs of other derivatives)
\end{align*}

\subsection{Théorèmes utiles} %TODO: completer si necessaire

\subsubsection{Théorème de la valeur intermédiaire}
Soit \(a < b \in \mathbb{R}; f : \left [a, b \right ] \to F\) une fonction continue. \\
Alors, \(f\) atteint son \(\sup_{\left [a, b \right ]}\), son \(\inf_{\left [a, b \right ]}\), et toutes les valeurs comprises entre \(f(a)\) et \(f(b)\).

\subsubsection{Théorème de Rolle}
Soit \(f : \left [a, b \right ] \to \mathbb{R}\). Alors:
\begin{equation*}
	\begin{rcases}
		f \text{ continue sur } \left [a, b \right ] \\
		f \text{ dérivable sur} \left [a, b \right ] \\
		f(a) = f(b)
	\end{rcases}
	\Rightarrow \exists c \in \left ]a, b \right [ : f'(c) = 0
\end{equation*}

\subsubsection{Théorème des accroissements finis (TAF)}
Soit \(f : \left [a, b \right ] \to \mathbb{R}\). Alors:
\begin{equation*}
	\begin{rcases}
		f \text{ continue sur } \left [a, b \right ] \\
		f \text{ dérivable sur } \left [a, b \right ] 
	\end{rcases}
	\Rightarrow \exists c \in \left ]a, b \right [ : f'(x) = \frac{f(b) - f(a)}{b - a}
\end{equation*}
On peut en tirer le corollaire suivant, plus utile:
\begin{equation*}
	f(x + h) = f(x) + h \cdot f'(x + \alpha h)
\end{equation*}

\subsubsection{Théorème de Bernouilli-l'Hospital}
Le théorème de Bernouilli-l'Hospital est utile lors de la recherche de limite d'une fonction rationnelle qui est a première vue indéterminée. \\

Il dit que la limite lorsque \(x\) tend vers \(a\) du quotient de deux fonctions dérivables en \(a\) est égale à la limite du quotient des dérivées de ces deux fonctions:
\begin{equation*}
	\lim_{x \to a} \dfrac{f(x)}{g(x)} = \lim_{x \to a} \dfrac{f'(x)}{g'(x)}
\end{equation*}
Cette identité peut être utilisée plusieurs fois si les fonction sont plusieurs fois dérivables en \(a\). \\

Le théorème de Bernouilli-l'Hospital peut souvent être utilisé sur des fonctions qui n'ont pas l'air rationnelles à première vue; par exemple, la fonction \(x \cdot \log(x)\) peut être écrite \(\frac{\log(x)}{\frac{1}{x}}\).
\begin{align*}
	\lim_{x \to 0} x \cdot \log(x) &= \lim_{x \to 0} \frac{\log(x)}{\frac{1}{x}} 
	\eqbl \lim_{x \to 0} \frac{\frac{1}{x}}{- \frac{1}{x^2}} 
	= - \lim_{x \to 0} \frac{x^2}{x} 
	= - \lim_{x \to 0} x = 0
\end{align*}

\textbf{Attention}: la limite de \(\dfrac{f'(x)}{g'(x)}\) n'existe pas \(\nRightarrow\) la limite de \(\dfrac{f(x)}{g(x)}\) n'existe pas.

\subsection{Développement limité}
Stop! Avant quoique ce soit, allez voir \href{https://www.youtube.com/watch?v=3d6DsjIBzJ4}{la vidéo de 3Blue1Brown sur le sujet des développements limités} (appelés \emph{Taylor teries} ou \emph{Taylor approximations} en anglais). Cela vous donnera une excellente compréhension de ce que sont les développements limités, et pourquoi ils sont définis ainsi. %TODO: completer

\section{Étude de fonctions}
Lorsqu'on étudie une fonction, on tente de connaître un maximum de ses propriétés. La liste des étapes à suivre lors de l'étude d'une fonction est généralement du type:
\begin{itemize}
	\item domaine de définition
	\item signe de la fonction
	\item extremums
	\item points de flexion, convexité et concavité
	\item asymptotes
	\item ... %TODO: completer
\end{itemize}

Dans cette section du document, pour des raisons de simplicité, nous appellerons "\(f(x)\)" la fonction réelle à étudier. Nous assumerons que ses dérivées existent lorsque nous les utiliserons. \\
Ce qui est entendu par les "zéros" d'une fonction est les valeurs de \(x\) pour lesquelles la valeur de la fonction est nulle. Graphiquement, ce sont les points sur l'axe des abscisses où cet axe est touché ou traversé par le graphe de la fonction. \\
Il y a donc deux types de zéros d'une fonction: 
\begin{itemize}
	\item ceux où le graphe touche simplement l'axe de la variable et repart ensuite dans la direction d'où il est venu (sans changer de signe), que nous appellerons \textbf{zéros faibles}, %TODO: trouver un meilleur nom 
	\item ceux où le graphe traverse l'axe des abscisses et continue de l'autre côté (ce qui signifie que le signe de la fonction est inversé), que nous appelerons \textbf{zéros forts}.
\end{itemize}
\emph{N.B: les termes "zéro faible" et "zéro fort" sont uniquement définis dans ce document à des fins de clarité, et ne sont pas officiellement définis dans le cours en général.}

\subsection{Étude de la fonction originale}

\subsubsection{Domaine de définition}
Pour trouver le domaine de définition d'une fonction réelle \(f\), il faut éliminer de \(\mathbb{R}\) toutes les valeurs pour lesquelles la fonction n'est pas définie. Il faut donc faire en sorte qu'aucune division par zéro ne puisse arriver, que les racines n'entourent pas de nombre négatif, que les logarithmes n'entourent pas de nombre négatif ou nul, etc. Après avoir déterminé ces valeurs / intervalles "interdites", on écrit le domaine de définition de la fonction \(f\) ainsi:
\begin{equation*}
	D_f = \mathbb{R} \setminus \{ x \in \mathbb{R} : x \text{ est une valeur "interdite"} \}
\end{equation*}

\(D_f\) est donc l'ensembles des valeurs pour lesquelles 

\subsubsection{Signe de la fonction}
Étudier le signe de la fonction \(f\) signifie déterminer sur quels intervalles la fonction est positive ou négative. La meilleure façon de procéder est de déterminer les zéros de la fonction, et de regarder quel est le signe de la fonction sur les intervalles délimités par les zéros. Comme défini plus haut, les zéros forts sont les zéros qui marquent un changement de signe de la fonction, et les zéros faibles sont ceux où le signe ne change pas.

\subsection{Étude des dérivées de la fonction}

\subsubsection{Extremums locaux}
Mathématiquement, les extremums locaux sont des points où \(f'\) est nulle. Attention, un point où \(f'\) est nulle \emph{n'est pas forcément un extremum local (c'est le cas des zéros faibles)}. \\

La condition proposée ci-dessous est suffisante à l'existence d'un extremum local de \(f\) en \(x = c\):
Soit \(c \in I\), et soit \(n\) pair, tels que:
\begin{align*}
	&f'(c) = f''(c) = f^{(3)}(c) = ... = f^{(n-1)}(c) = 0, \\ 
	&\text{ mais } f^{(n)}(c) \neq 0, \text{ alors:} \\
	& &\begin{cases}
		f \text{ admet un minimum local en } x = c \text{ si } \quad f^{(n)}(c) > 0  \\
		f \text{ admet un maximum local en } x = c \text{ si } \quad f^{(n)}(c) < 0  
	\end{cases}
\end{align*}
Cette condition est ici définie de façon très générale, mais il suffit généralement d'étudier les première et seconde dérivées de \(f\), voire les troisième et quatrième dans le pire des cas, afin de définir s'il existe un extremum local en \(x = c\). \\
Afin de se souvenir pourquoi on a un minimum lorsque la \(n^\text{eme}\) dérivée est positive, et vice-versa, on peut penser par exemple:
\begin{itemize}
	\item au graphe de \(x^2\), dont la \(2^\text{nde}\) dérivée est \(2 > 0\), et qui admet un minimum en \(x = 0\).
	\item au graphe de \(-x^2\), dont la \(2^\text{nde}\) dérivée est  \(-2 < 0\), et qui admet un mimimum en \(x = 0\).
\end{itemize}

Une autre méthode pour déterminer les extremums locaux consiste à trouver les zéros forts de \(f'\), qui seront donc les points où \(f'\) change de signe, et donc les extremums locaux. Il faut donc d'abord déterminer tous les zéros de \(f'\), puis étudier le signe de \(f'\) autour de ces points, afin de déterminer quels zéros sont forts/faibles, et, pour les zéros forts, si il s'agit de maximums ou de minimums de \(f\).

\subsubsection{Points de flexion, convexité et concavité}
Les points de flexion, la convexité et la concavité sont étudiées avec l'aide de la dérivée seconde de la fonction. \\

Sur un intervalle \(I\), une fonction est:
\begin{itemize}
	\item \textbf{convexe}, si \(\forall x \in I, f''(x) \geq 0\)
	\item \textbf{concave}, si \(\forall x \in I, f''(x) \leq 0\)
\end{itemize}
On voit que lorsque \(\forall x \in I f''(x) = 0\), le graphe est à la fois convexe et concave: autrement dit, le graphe est linéaire sur cet intervalle.

Graphiquement, "convexe" signifie que le graphe est "plié vers le haut" (un peu comme un bol), et concave signifie à l'inverse "plié vers le bas". Cependant, la convexité/concavité n'est pas toujours aussi évidente lorsqu'on observe simplement le graphe d'une fonction. \\

Les points de flexions sont les points où la fonction passe de concave à convexe, ou inversément. De manière analogue aux extremums, les points de flexions sont des points ou \(f''\) est nulle. Encore une fois, un point ou \(f''\) est nulle \emph{n'est pas nécessairement un point de flexion}. \\

\section{Séries entières}

\subsection{Rayon de convergence}

\section{Annexes} \label{sec:annexes}

\subsection{Identités des fonctions trigonométriques hyperboliques} \label{sec:hypertrigo}

\begin{align*}
	&\sinh(x \pm y) = \sinh(x)\cosh(y) \pm \cosh(x)\sinh(y) & &\cosh(x \pm y) = \cosh(x)\cosh(y) \pm \sinh(x)\sinh(y) \\
	&\begin{cases}
		\sinh(2x) = 2\sinh(x)\cosh(x)
	\end{cases} & &\begin{cases}
		\begin{aligned}
			\cosh(2x) &= \cosh^2(x) + \sinh^2(x) \\
						&= 2\cosh^2(x) - 1 \\
						&= 1 + 2\sinh^2(x) 
		\end{aligned}
	\end{cases} \\
	&\cosh^2(x) - \sinh^2(x) = 1 \\
	&\tanh(x) = \frac{\sinh(x)}{\cosh(x)} \\
	&\sinh(-x) = -\sin(x) \quad \text{(impaire)} & &\cosh(-x) = \cosh(x) \quad \text{(paire)}
\end{align*}


\end{document}

















